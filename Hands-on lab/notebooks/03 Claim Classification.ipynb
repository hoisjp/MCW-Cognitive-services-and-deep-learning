{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kerasによるクレーム分類：Pythonディープラーニングライブラリ\n",
    "\n",
    "このノートブックでは、請求が自動車保険請求である場合は `1` 、住宅保険請求である場合は `0` を予測する請求テキストの分類モデルをトレーニングします。 モデルは、Kerasライブラリを介してTensorFlowを使用し、Long Short-Term Memory（LSTM）リカレントニューラルネットワークと呼ばれるDNNのタイプを使用して構築されます。\n",
    "\n",
    "このノートブックでは、以下で構成されるテキスト分析プロセスについて説明します。\n",
    "\n",
    "- GloVe単語の埋め込みを使用した例の類推\n",
    "- GloVe単語の埋め込みを使用したトレーニングデータのベクトル化\n",
    "- LSTMベースの分類モデルの作成とトレーニング\n",
    "- モデルを使用して分類を予測する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モジュールを準備する\n",
    "\n",
    "このノートブックは、Kerasライブラリを使用して分類器を作成およびトレーニングします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print('Keras version: ', keras.__version__)\n",
    "print('Tensorflow version: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**事前トレーニング済みのGloVe単語の埋め込みをダウンロードして、このノートブックにロードしてみましょう。**\n",
    "\n",
    "これで、サイズが **400,000** ワードの `dictionary`と、辞書内の単語に対応する ` GloVe wordベクトル ` が作成されます。 各単語ベクトルのサイズは50であるため、ここで使用される単語埋め込みの次元は **50** です。\n",
    "\n",
    "*次のセルの実行には数分かかる場合があります*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                  'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordsList.npy')\n",
    "\n",
    "word_vectors_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordVectors.npy')\n",
    "\n",
    "word_vectors_dir = './word_vectors'\n",
    "\n",
    "os.makedirs(word_vectors_dir, exist_ok=True)\n",
    "urllib.request.urlretrieve(words_list_url, os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "urllib.request.urlretrieve(word_vectors_url, os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "\n",
    "dictionary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "dictionary = dictionary.tolist()\n",
    "dictionary = [word.decode('UTF-8') for word in dictionary]\n",
    "print('Loaded the dictionary! Dictionary size: ', len(dictionary))\n",
    "\n",
    "word_vectors = np.load(os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "print ('Loaded the word vectors! Shape of the word vectors: ', word_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**収縮マップという単語を作成します。 マップはコーパスの縮約を拡張するために使用されます（たとえば、 \"can't\" は\"cannot\"になります）。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/contractions.xlsx')\n",
    "contractions_df = pd.read_excel(contractions_url)\n",
    "contractions = dict(zip(contractions_df.original, contractions_df.expanded))\n",
    "print('Review first 10 entries from the contractions map')\n",
    "print(contractions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe単語埋め込みを使用したワードアナロジーの例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVeは、辞書内の各単語をベクトルとして表します。 単語の類推を予測するために単語ベクトルを使用できます。\n",
    "\n",
    "次の類推を解決する以下の例を参照してください。 **man と woman の関係は、king と\"何\"の関係に対応するか？**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コサイン類似度は、2つの単語の類似度を評価するために使用される尺度です。 このヘルパー関数は、2つの単語のベクトルを取り、-1〜1の範囲のコサイン類似度を返します。同義語の場合、コサイン類似度は1に近く、反意語の場合、コサイン類似度は-1に近くなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    dot = u.dot(v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    cosine_similarity = dot/norm_u/norm_v\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語のベクトルを確認してみましょう **man**, **woman**, and **king**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man = word_vectors[dictionary.index('man')]\n",
    "woman = word_vectors[dictionary.index('woman')]\n",
    "king = word_vectors[dictionary.index('king')]\n",
    "print(man)\n",
    "print('')\n",
    "print(woman)\n",
    "print('')\n",
    "print(king)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "類推を解くには、次の方程式でxを解く必要があります。\n",
    "\n",
    "**woman – man = x - king**\n",
    "\n",
    "したがって, **x = woman - man + king**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = woman - man + king"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**次に、単語ベクトルが上記で計算されたベクトルxに最も近い単語を見つけます。**\n",
    "\n",
    "計算コストを制限するために、辞書全体を検索するのではなく、可能な回答のリストから最適な単語を特定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = ['woman', 'prince', 'princess', 'england', 'goddess', 'diva', 'empress', \n",
    "           'female', 'lady', 'monarch', 'title', 'queen', 'sovereign', 'ruler', \n",
    "           'male', 'crown', 'majesty', 'royal', 'cleopatra', 'elizabeth', 'victoria', \n",
    "           'throne', 'internet', 'sky', 'machine', 'learning', 'fairy']\n",
    "\n",
    "df = pd.DataFrame(columns = ['word', 'cosine_similarity'])\n",
    "\n",
    "# Find the similarity of each word in answers with x\n",
    "for w in answers:\n",
    "    sim = cosine_similarity(word_vectors[dictionary.index(w)], x)   \n",
    "    df = df.append({'word': w, 'cosine_similarity': sim}, ignore_index=True)\n",
    "    \n",
    "df.sort_values(['cosine_similarity'], ascending=False, inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**上記の結果から、単語「queen」のベクトルがベクトル「x」に最も類似していることがわかります。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数値ばかりで分かりにくいでしょうか？\n",
    "\n",
    "**【補講】**\n",
    "単語ベクトル表現のさらなる理解のために、ベクトルを可視化してみましょう。\n",
    "多次元ベクトルを可視化するためには、次元削減という手法を用います。PCA(主成分分析)を使用すると、元のベクトルが持つ情報をなるべく欠損することなく縮約することができます。今回は50次元のデータを2次元に減らします。これを実行すれば、matplotlib散布図を使用してプロットすることができるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ベクトル同士の演算\n",
    "man = word_vectors[dictionary.index('man')]\n",
    "woman = word_vectors[dictionary.index('woman')]\n",
    "king = word_vectors[dictionary.index('king')]\n",
    "x = woman - man + king\n",
    "\n",
    "words = ['king', 'man', 'woman', 'queen']\n",
    "# GloVe 辞書から指定した単語のベクトルを取り出す\n",
    "vectors = [word_vectors[dictionary.index(word)] for word in words]\n",
    "\n",
    "# ベクトル演算結果の確認のため、配列の末尾に演算結果xを追加\n",
    "vectors.append(x)\n",
    "words.append('x')\n",
    "\n",
    "# 50次元を2次元に削減\n",
    "twodim = PCA(n_components=2).fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "# 散布図を作成\n",
    "plt.scatter(twodim[:, 0], twodim[:, 1], edgecolors='k', c='b')\n",
    "\n",
    "for label, x, y in zip(words, twodim[:, 0], twodim[:, 1]):\n",
    "    labeltitle = label + \"(\" + str(np.round(x, decimals=1)) + \",\" + str(np.round(y, decimals=1)) + \")\"\n",
    "    plt.annotate(labeltitle, xy=(x, y), xytext=(5, 0), textcoords=\"offset points\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「王」から「男性」を引いて「女性」を足したとき、それは「女王」となるであろうという類推が、GloVeベクトルの演算で可能になりました。これは何を意味するかというと、単語間の意味的な関係をベクトルで表現することができるということです。この技術の登場によって、単語をニューラルネットワークのインプットにすることができるようになり、機械翻訳や文章の固有表現抽出など幅広い領域に用いられるようになりました。\n",
    "\n",
    "それではもっと面白い特徴をお見せしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトル同士の演算\n",
    "man = word_vectors[dictionary.index('man')]\n",
    "woman = word_vectors[dictionary.index('woman')]\n",
    "king = word_vectors[dictionary.index('king')]\n",
    "x = woman - man + king\n",
    "\n",
    "# GloVe 辞書内に含まれる単語をカテゴリごとに追加（自由に編集してください）\n",
    "words = ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', \n",
    "         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
    "         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
    "         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
    "         'france', 'germany', 'hungary', 'luxembourg', 'australia', 'japan',\n",
    "         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
    "         'school', 'college', 'university', 'institute',\n",
    "         'king', 'man', 'woman', 'queen']\n",
    "\n",
    "# GloVe 辞書から指定した単語のベクトルを取り出す\n",
    "vectors = [word_vectors[dictionary.index(word)] for word in words]\n",
    "\n",
    "# ベクトル演算結果の確認のため、配列の末尾に演算結果xを追加\n",
    "vectors.append(x)\n",
    "words.append('x')\n",
    "\n",
    "# 50次元を2次元に削減\n",
    "twodim = PCA(n_components=2).fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "# 散布図を作成\n",
    "plt.scatter(twodim[:, 0], twodim[:, 1], edgecolors='k', c='b')\n",
    "\n",
    "for label, x, y in zip(words, twodim[:, 0], twodim[:, 1]):\n",
    "    labeltitle = label + \"(\" + str(np.round(x, decimals=1)) + \",\" + str(np.round(y, decimals=1)) + \")\"\n",
    "    plt.annotate(labeltitle, xy=(x, y), xytext=(5, 0), textcoords=\"offset points\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "似た意味の単語同士がベクトル空間上で近い位置に配置されることがグラフから分かりましたか？これは魔法のようなことです！🧙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレーニングデータを準備する\n",
    "\n",
    "Contoso Ltdは、クレームテキストとして受け取るテキストの例を含む小さなドキュメントを提供しています。 彼らは、これをサンプルクレームごとに1行のテキストファイルで提供しています。\n",
    "\n",
    "次のセルを実行して、ファイルの内容をダウンロードして確認します。 少し時間をかけてクレームを読んでください（あなたはそれらのいくつかをかなりコミカルに感じるかもしれません！）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = './data'\n",
    "base_data_url = 'https://databricksdemostore.blob.core.windows.net/data/05.03/'\n",
    "filesToDownload = ['claims_text.txt', 'claims_labels.txt']\n",
    "\n",
    "os.makedirs(data_location, exist_ok=True)\n",
    "\n",
    "for file in filesToDownload:\n",
    "    data_url = os.path.join(base_data_url, file)\n",
    "    local_file_path = os.path.join(data_location, file)\n",
    "    urllib.request.urlretrieve(data_url, local_file_path)\n",
    "    print('Downloaded file: ', file)\n",
    "    \n",
    "claims_corpus = [claim for claim in open(os.path.join(data_location, 'claims_text.txt'))]\n",
    "claims_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クレームサンプルに加えて、Contoso Ltdは、提供されたサンプルクレームのそれぞれに0（「住宅保険クレーム」）または1（「自動車保険クレーム」）のいずれかとしてラベルを付けるドキュメントも提供しています。 これは、サンプルごとに1行のテキストファイルとして提示され、クレームテキストと同じ順序で提示されます。\n",
    "\n",
    "次のセルを実行して、提供されたClaims_labels.txtファイルの内容を調べます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [int(re.sub(\"\\n\", \"\", label)) for label in open(os.path.join(data_location, 'claims_labels.txt'))]\n",
    "print(len(labels))\n",
    "print(labels[0:5]) # first 5 labels\n",
    "print(labels[-5:]) # last 5 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の出力からわかるように、値は整数0または1です。これらをモデルのトレーニングに使用するラベルとして使用するには、これらの整数値をカテゴリ値に変換する必要があります（他のプログラミングの列挙型のようなものと考えてください） 言語）。\n",
    "\n",
    "`keras.utils` の to_categorical メソッドを使用して、これらの値をバイナリのカテゴリ値に変換できます。 次のセルを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(labels, 2)\n",
    "print(labels.shape)\n",
    "print()\n",
    "print(labels[0:2]) # first 2 categorical labels\n",
    "print()\n",
    "print(labels[-2:]) # last 2 categorical labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クレームテキストとラベルが読み込まれたので、テキスト分析プロセスの最初のステップ、つまりテキストを正規化する準備ができました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クレームコーパスを処理する\n",
    "\n",
    "- すべての単語を小文字にします\n",
    "- 収縮を拡張する（たとえば、 \"can't\" が \"cannot\" になる）\n",
    "- 特殊文字（句読点など）を削除する\n",
    "- クレームテキスト内の単語のリストを、辞書内のそれらの単語の対応するインデックスのリストに変換します。 書面のクレームに表示される単語の順序は維持されることに注意してください。\n",
    "\n",
    "次のセルを実行して、クレームコーパスを処理します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(token):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_token = pattern.sub('', token)\n",
    "    return filtered_token\n",
    "\n",
    "def convert_to_indices(corpus, dictionary, c_map, unk_word_index = 399999):\n",
    "    sequences = []\n",
    "    for i in range(len(corpus)):\n",
    "        tokens = corpus[i].split()\n",
    "        sequence = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word in c_map:\n",
    "                resolved_words = c_map[word].split()\n",
    "                for resolved_word in resolved_words:\n",
    "                    try:\n",
    "                        word_index = dictionary.index(resolved_word)\n",
    "                        sequence.append(word_index)\n",
    "                    except ValueError:\n",
    "                        sequence.append(unk_word_index) #Vector for unkown words\n",
    "            else:\n",
    "                try:\n",
    "                    clean_word = remove_special_characters(word)\n",
    "                    if len(clean_word) > 0:\n",
    "                        word_index = dictionary.index(clean_word)\n",
    "                        sequence.append(word_index)\n",
    "                except ValueError:\n",
    "                    sequence.append(unk_word_index) #Vector for unkown words\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "claims_corpus_indices = convert_to_indices(claims_corpus, dictionary, contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1つのサンプルクレームのインデックスを確認する**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_special_characters(claims_corpus[5]).split())\n",
    "print()\n",
    "print('Ordered list of indices for the above claim')\n",
    "print(claims_corpus_indices[5])\n",
    "print('')\n",
    "print('For example, the index of second word in the claims text \\\"pedestrian\\\" is: ', dictionary.index('pedestrian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**固定長ベクトルを作成する**\n",
    "\n",
    "クレームで使用される単語の数は、クレームによって異なります。 固定サイズの入力ベクトルを作成する必要があります。 `keras.preprocessing.sequence` のユーティリティ関数 ` pad_sequences` を使用して、単語インデックスの固定サイズのベクトル（サイズ= 125）を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 125\n",
    "\n",
    "X = pad_sequences(claims_corpus_indices, maxlen=maxSeqLength, padding='pre', truncating='post')\n",
    "\n",
    "print('Review the new fixed size vector for a sample claim')\n",
    "print(remove_special_characters(claims_corpus[5]).split())\n",
    "print()\n",
    "print(X[5])\n",
    "print('')\n",
    "print('Lenght of the vector: ', len(X[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMリカレントニューラルネットワークを構築する\n",
    "\n",
    "テキストデータのトレーニングから入力特徴を前処理したので、分類器を作成する準備ができました。 この場合、LSTMリカレントニューラルネットワークを構築します。 ネットワークには、単語インデックスをGloVe単語ベクトルに変換する単語埋め込みレイヤーがあります。 次に、GloVeワードベクトルがLSTM層に渡され、その後に二項分類出力層が続きます。\n",
    "\n",
    "次のセルを実行して、ニューラルネットワークの構造を構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(word_vectors.shape[0],\n",
    "                            word_vectors.shape[1],\n",
    "                            weights=[word_vectors],\n",
    "                            input_length=maxSeqLength,\n",
    "                            trainable=False)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークをトレーニングする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初に、データを2つのセットに分割します：（1）トレーニングセットと（2）検証またはテストセット。 検証セットの精度は、モデルのパフォーマンスを測定するために使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `Adam` 最適化アルゴリズムを使用してモデルをトレーニングします。 また、問題のタイプが `Binary Classification` であるため、出力層には `Sigmoid` アクティベーション関数を使用し、損失関数には `Binary Crossentropy` を使用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、トレーニングデータとラベルに合わせてDNNに学習させる準備ができました。 トレーニングのバッチサイズとエポック数を定義しました。\n",
    "\n",
    "次のセルを実行して、モデルをデータに合わせます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 16\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "値「val_accuracy」の最終出力を見てください。 これは、検証セットの精度を表しています。 ランダムチャンスを50％の精度であると考える場合、モデルはランダムよりも優れていますか？\n",
    "\n",
    "この時点でランダムよりも良くない場合でも問題ありません-これは初めてのモデルです！ 典型的なデータサイエンスプロセスは、モデルの精度を向上させるために、次のようなさまざまなアクションを繰り返し実行します。\n",
    "- トレーニング用にさらにラベルが付けられたドキュメントを取得する\n",
    "- 過適合を防ぐための正則化\n",
    "- レイヤー数、レイヤーあたりのノード数、学習率などのモデルハイパーパラメーターの調整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類クレームのテスト\n",
    "\n",
    "モデルを構築したので、一連のクレームに対してそれを試してください。 最初にテキストを前処理する必要があることを思い出してください。\n",
    "\n",
    "次のセルを実行して、テストデータを準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_claim = ['I crashed my car into a pole.', \n",
    "              'The flood ruined my house.', \n",
    "              'I lost control of my car and fell in the river.']\n",
    "\n",
    "test_claim_indices = convert_to_indices(test_claim, dictionary, contractions)\n",
    "test_data = pad_sequences(test_claim_indices, maxlen=maxSeqLength, padding='pre', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、モデルを使用して分類を予測します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)\n",
    "pred_label = pred.argmax(axis=1)\n",
    "pred_df = pd.DataFrame(np.column_stack((pred,pred_label)), columns=['class_0', 'class_1', 'label'])\n",
    "pred_df.label = pred_df.label.astype(int)\n",
    "print('Predictions')\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルのエクスポートとインポート\n",
    "\n",
    "これで作業モデルができたので、トレーニング済みモデルをファイルにエクスポートして、デプロイされたWebサービスがダウンストリームで使用できるようにする必要があります。\n",
    "\n",
    "*次の2つのセルの実行には数分かかる場合があります*\n",
    "\n",
    "モデルをエクスポートするには、次のセルを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "output_folder = './output'\n",
    "model_filename = 'final_model.hdf5'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "model.save(os.path.join(output_folder, model_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じNotebookインスタンスへのモデルの再ロードをテストするには、次のセルを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = load_model(os.path.join(output_folder, model_filename))\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前と同じように、モデルを使用して予測を実行できます。\n",
    "\n",
    "次のセルを実行して、再ロードされたモデルで予測を試みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = loaded_model.predict(test_data)\n",
    "pred_label = pred.argmax(axis=1)\n",
    "pred_df = pd.DataFrame(np.column_stack((pred,pred_label)), columns=['class_0', 'class_1', 'label'])\n",
    "pred_df.label = pred_df.label.astype(int)\n",
    "print('Predictions')\n",
    "pred_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python36",
   "display_name": "Python 3.6",
   "language": "python"
  },
  "language_info": {
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6",
   "file_extension": ".py",
   "codemirror_mode": {
    "version": 3,
    "name": "ipython"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}