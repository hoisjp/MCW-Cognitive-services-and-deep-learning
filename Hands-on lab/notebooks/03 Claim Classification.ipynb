{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kerasã«ã‚ˆã‚‹ã‚¯ãƒ¬ãƒ¼ãƒ åˆ†é¡ï¼šPythonãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€è«‹æ±‚ãŒè‡ªå‹•è»Šä¿é™ºè«‹æ±‚ã§ã‚ã‚‹å ´åˆã¯ `1` ã€ä½å®…ä¿é™ºè«‹æ±‚ã§ã‚ã‚‹å ´åˆã¯ `0` ã‚’äºˆæ¸¬ã™ã‚‹è«‹æ±‚ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚ ãƒ¢ãƒ‡ãƒ«ã¯ã€Kerasãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä»‹ã—ã¦TensorFlowã‚’ä½¿ç”¨ã—ã€Long Short-Term Memoryï¼ˆLSTMï¼‰ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨å‘¼ã°ã‚Œã‚‹DNNã®ã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨ã—ã¦æ§‹ç¯‰ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ä»¥ä¸‹ã§æ§‹æˆã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚\n",
    "\n",
    "- GloVeå˜èªåŸ‹ã‚è¾¼ã¿ã¨å˜èªã‚¢ãƒŠãƒ­ã‚¸ãƒ¼ã®ä¾‹\n",
    "- GloVeå˜èªã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–\n",
    "- LSTMãƒ™ãƒ¼ã‚¹ã®åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "- ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æŒ‡æ¨™ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç›£è¦–ã™ã‚‹\n",
    "- ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦åˆ†é¡ã‚’äºˆæ¸¬ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æº–å‚™ã™ã‚‹\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Kerasãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦åˆ†é¡å™¨ã‚’ä½œæˆãŠã‚ˆã³ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print('Keras version: ', keras.__version__)\n",
    "print('Tensorflow version: ', tf.__version__)\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Experiment, Workspace, Run, Datastore\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.model import Model\n",
    "from azureml.train.dnn import TensorFlow\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "print(\"Azure ML SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®GloVeå˜èªã®åŸ‹ã‚è¾¼ã¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«ãƒ­ãƒ¼ãƒ‰ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚**\n",
    "\n",
    "ã“ã‚Œã§ã€ã‚µã‚¤ã‚ºãŒ **400,000** ãƒ¯ãƒ¼ãƒ‰ã® `dictionary`ã¨ã€è¾æ›¸å†…ã®å˜èªã«å¯¾å¿œã™ã‚‹ ` GloVe wordãƒ™ã‚¯ãƒˆãƒ« ` ãŒä½œæˆã•ã‚Œã¾ã™ã€‚ å„å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®ã‚µã‚¤ã‚ºã¯50ã§ã‚ã‚‹ãŸã‚ã€ã“ã“ã§ä½¿ç”¨ã•ã‚Œã‚‹å˜èªåŸ‹ã‚è¾¼ã¿ã®æ¬¡å…ƒã¯ **50** ã§ã™ã€‚\n",
    "\n",
    "*æ¬¡ã®ã‚»ãƒ«ã®å®Ÿè¡Œã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                  'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordsList.npy')\n",
    "\n",
    "word_vectors_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordVectors.npy')\n",
    "\n",
    "word_vectors_dir = './word_vectors'\n",
    "\n",
    "os.makedirs(word_vectors_dir, exist_ok=True)\n",
    "urllib.request.urlretrieve(words_list_url, os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "urllib.request.urlretrieve(word_vectors_url, os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "\n",
    "dictionary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "dictionary = dictionary.tolist()\n",
    "dictionary = [word.decode('UTF-8') for word in dictionary]\n",
    "print('Loaded the dictionary! Dictionary size: ', len(dictionary))\n",
    "\n",
    "word_vectors = np.load(os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "print ('Loaded the word vectors! Shape of the word vectors: ', word_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**åç¸®ãƒãƒƒãƒ—ã¨ã„ã†å˜èªã‚’ä½œæˆã—ã¾ã™ã€‚ ãƒãƒƒãƒ—ã¯ã‚³ãƒ¼ãƒ‘ã‚¹ã®ç¸®ç´„ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ï¼ˆãŸã¨ãˆã°ã€ \"can't\" ã¯\"cannot\"ã«ãªã‚Šã¾ã™ï¼‰ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/contractions.xlsx')\n",
    "contractions_df = pd.read_excel(contractions_url)\n",
    "contractions = dict(zip(contractions_df.original, contractions_df.expanded))\n",
    "print('Review first 10 entries from the contractions map')\n",
    "print(contractions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVeå˜èªåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã—ãŸãƒ¯ãƒ¼ãƒ‰ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼ã®ä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVeã¯ã€è¾æ›¸å†…ã®å„å˜èªã‚’ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦è¡¨ã—ã¾ã™ã€‚ãƒ¯ãƒ¼ãƒ‰ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã«å˜èªãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚\n",
    "\n",
    "æ¬¡ã®é¡æ¨ã‚’è§£æ±ºã™ã‚‹ä»¥ä¸‹ã®ä¾‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ **man ã¨ woman ã®é–¢ä¿‚ã¯ã€king ã¨\"ä½•\"ã®é–¢ä¿‚ã«å¯¾å¿œã™ã‚‹ã‹ï¼Ÿ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã¯ã€2ã¤ã®å˜èªã®é¡ä¼¼åº¦ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹å°ºåº¦ã§ã™ã€‚ ã“ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã¯ã€2ã¤ã®å˜èªã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–ã‚Šã€-1ã€œ1ã®ç¯„å›²ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¿”ã—ã¾ã™ã€‚åŒç¾©èªã®å ´åˆã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã¯1ã«è¿‘ãã€åæ„èªã®å ´åˆã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã¯-1ã«è¿‘ããªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    dot = u.dot(v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    cosine_similarity = dot/norm_u/norm_v\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å˜èªã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã† **man**, **woman**, and **king**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man = word_vectors[dictionary.index('man')]\n",
    "woman = word_vectors[dictionary.index('woman')]\n",
    "king = word_vectors[dictionary.index('king')]\n",
    "print(man)\n",
    "print('')\n",
    "print(woman)\n",
    "print('')\n",
    "print(king)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¡æ¨ã‚’è§£ãã«ã¯ã€æ¬¡ã®æ–¹ç¨‹å¼ã§xã‚’è§£ãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "**woman â€“ man = x - king**\n",
    "\n",
    "ã—ãŸãŒã£ã¦, **x = woman - man + king**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = woman - man + king"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ¬¡ã«ã€å˜èªãƒ™ã‚¯ãƒˆãƒ«ãŒä¸Šè¨˜ã§è¨ˆç®—ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«xã«æœ€ã‚‚è¿‘ã„å˜èªã‚’è¦‹ã¤ã‘ã¾ã™ã€‚**\n",
    "\n",
    "è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’åˆ¶é™ã™ã‚‹ãŸã‚ã«ã€è¾æ›¸å…¨ä½“ã‚’æ¤œç´¢ã™ã‚‹ã®ã§ã¯ãªãã€å¯èƒ½ãªå›ç­”ã®ãƒªã‚¹ãƒˆã‹ã‚‰æœ€é©ãªå˜èªã‚’ç‰¹å®šã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = ['woman', 'prince', 'princess', 'england', 'goddess', 'diva', 'empress', \n",
    "           'female', 'lady', 'monarch', 'title', 'queen', 'sovereign', 'ruler', \n",
    "           'male', 'crown', 'majesty', 'royal', 'cleopatra', 'elizabeth', 'victoria', \n",
    "           'throne', 'internet', 'sky', 'machine', 'learning', 'fairy']\n",
    "\n",
    "df = pd.DataFrame(columns = ['word', 'cosine_similarity'])\n",
    "\n",
    "# Find the similarity of each word in answers with x\n",
    "for w in answers:\n",
    "    sim = cosine_similarity(word_vectors[dictionary.index(w)], x)   \n",
    "    df = df.append({'word': w, 'cosine_similarity': sim}, ignore_index=True)\n",
    "    \n",
    "df.sort_values(['cosine_similarity'], ascending=False, inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ä¸Šè¨˜ã®çµæœã‹ã‚‰ã€å˜èªã€Œqueenã€ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒãƒ™ã‚¯ãƒˆãƒ«ã€Œxã€ã«æœ€ã‚‚é¡ä¼¼ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ•°å€¤ã°ã‹ã‚Šã§åˆ†ã‹ã‚Šã«ãã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n",
    "\n",
    "**ã€è£œè¬›ã€‘**\n",
    "å˜èªãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã®ã•ã‚‰ãªã‚‹ç†è§£ã®ãŸã‚ã«ã€ãƒ™ã‚¯ãƒˆãƒ«ã‚’å¯è¦–åŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
    "å¤šæ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’å¯è¦–åŒ–ã™ã‚‹ãŸã‚ã«ã¯ã€æ¬¡å…ƒå‰Šæ¸›ã¨ã„ã†æ‰‹æ³•ã‚’ç”¨ã„ã¾ã™ã€‚PCA(ä¸»æˆåˆ†åˆ†æ)ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ãŒæŒã¤æƒ…å ±ã‚’ãªã‚‹ã¹ãæ¬ æã™ã‚‹ã“ã¨ãªãç¸®ç´„ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä»Šå›ã¯50æ¬¡å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’2æ¬¡å…ƒã«æ¸›ã‚‰ã—ã¾ã™ã€‚ã“ã‚Œã‚’å®Ÿè¡Œã™ã‚Œã°ã€matplotlibæ•£å¸ƒå›³ã‚’ä½¿ç”¨ã—ã¦2æ¬¡å…ƒã‚°ãƒ©ãƒ•ä¸Šã«ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®æ¼”ç®—\n",
    "man = word_vectors[dictionary.index('man')]\n",
    "woman = word_vectors[dictionary.index('woman')]\n",
    "king = word_vectors[dictionary.index('king')]\n",
    "x = woman - man + king\n",
    "\n",
    "words = ['king', 'man', 'woman', 'queen']\n",
    "# GloVe è¾æ›¸ã‹ã‚‰æŒ‡å®šã—ãŸå˜èªã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–ã‚Šå‡ºã™\n",
    "vectors = [word_vectors[dictionary.index(word)] for word in words]\n",
    "\n",
    "# ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—çµæœã®ç¢ºèªã®ãŸã‚ã€é…åˆ—ã®æœ«å°¾ã«æ¼”ç®—çµæœxã‚’è¿½åŠ \n",
    "vectors.append(x)\n",
    "words.append('x')\n",
    "\n",
    "# 50æ¬¡å…ƒã‚’2æ¬¡å…ƒã«å‰Šæ¸›\n",
    "twodim = PCA(n_components=2).fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "# æ•£å¸ƒå›³ã‚’ä½œæˆ\n",
    "plt.scatter(twodim[:, 0], twodim[:, 1], edgecolors='k', c='b')\n",
    "\n",
    "for label, x, y in zip(words, twodim[:, 0], twodim[:, 1]):\n",
    "    labeltitle = label + \"(\" + str(np.round(x, decimals=1)) + \",\" + str(np.round(y, decimals=1)) + \")\"\n",
    "    plt.annotate(labeltitle, xy=(x, y), xytext=(5, 0), textcoords=\"offset points\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã€Œç‹ã€ã‹ã‚‰ã€Œç”·æ€§ã€ã‚’å¼•ã„ã¦ã€Œå¥³æ€§ã€ã‚’è¶³ã—ãŸã¨ãã€ãã‚Œã¯ã€Œå¥³ç‹ã€ã¨ãªã‚‹ã§ã‚ã‚ã†ã¨ã„ã†é¡æ¨ãŒã€GloVeãƒ™ã‚¯ãƒˆãƒ«ã®æ¼”ç®—ã§å¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚ã“ã‚Œã¯ä½•ã‚’æ„å‘³ã™ã‚‹ã‹ã¨ã„ã†ã¨ã€å˜èªé–“ã®æ„å‘³çš„ãªé–¢ä¿‚ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ç¾ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚ã“ã®æŠ€è¡“ã®ç™»å ´ã«ã‚ˆã£ã¦ã€å˜èªã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã«ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã€æ©Ÿæ¢°ç¿»è¨³ã‚„æ–‡ç« ã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºãªã©å¹…åºƒã„é ˜åŸŸã«ç”¨ã„ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n",
    "\n",
    "ãã‚Œã§ã¯ã‚‚ã£ã¨é¢ç™½ã„ç‰¹å¾´ã‚’ãŠè¦‹ã›ã—ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®æ¼”ç®—\n",
    "man = word_vectors[dictionary.index('man')]\n",
    "woman = word_vectors[dictionary.index('woman')]\n",
    "king = word_vectors[dictionary.index('king')]\n",
    "x = woman - man + king\n",
    "\n",
    "# GloVe è¾æ›¸å†…ã«å«ã¾ã‚Œã‚‹å˜èªã‚’ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«è¿½åŠ ï¼ˆè‡ªç”±ã«ç·¨é›†ã—ã¦ãã ã•ã„ï¼‰\n",
    "words = ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', \n",
    "         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
    "         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
    "         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
    "         'france', 'germany', 'hungary', 'luxembourg', 'australia', 'japan',\n",
    "         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
    "         'school', 'college', 'university', 'institute',\n",
    "         'king', 'man', 'woman', 'queen']\n",
    "\n",
    "# GloVe è¾æ›¸ã‹ã‚‰æŒ‡å®šã—ãŸå˜èªã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–ã‚Šå‡ºã™\n",
    "vectors = [word_vectors[dictionary.index(word)] for word in words]\n",
    "\n",
    "# ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—çµæœã®ç¢ºèªã®ãŸã‚ã€é…åˆ—ã®æœ«å°¾ã«æ¼”ç®—çµæœxã‚’è¿½åŠ \n",
    "vectors.append(x)\n",
    "words.append('x')\n",
    "\n",
    "# 50æ¬¡å…ƒã‚’2æ¬¡å…ƒã«å‰Šæ¸›\n",
    "twodim = PCA(n_components=2).fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "# æ•£å¸ƒå›³ã‚’ä½œæˆ\n",
    "plt.scatter(twodim[:, 0], twodim[:, 1], edgecolors='k', c='b')\n",
    "\n",
    "for label, x, y in zip(words, twodim[:, 0], twodim[:, 1]):\n",
    "    labeltitle = label + \"(\" + str(np.round(x, decimals=1)) + \",\" + str(np.round(y, decimals=1)) + \")\"\n",
    "    plt.annotate(labeltitle, xy=(x, y), xytext=(5, 0), textcoords=\"offset points\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¼¼ãŸæ„å‘³ã®å˜èªåŒå£«ãŒãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ä¸Šã§è¿‘ã„ä½ç½®ã«é…ç½®ã•ã‚Œã‚‹ã“ã¨ãŒã‚°ãƒ©ãƒ•ã‹ã‚‰åˆ†ã‹ã‚Šã¾ã—ãŸã‹ï¼Ÿã“ã‚Œã¯é­”æ³•ã®ã‚ˆã†ãªã“ã¨ã§ã™ï¼ğŸ§™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹\n",
    "\n",
    "Contoso Ltdã¯ã€ã‚¯ãƒ¬ãƒ¼ãƒ ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å—ã‘å–ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ä¾‹ã‚’å«ã‚€å°ã•ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ å½¼ã‚‰ã¯ã€ã“ã‚Œã‚’ã‚µãƒ³ãƒ—ãƒ«ã‚¯ãƒ¬ãƒ¼ãƒ ã”ã¨ã«1è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã§æä¾›ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ç¢ºèªã—ã¾ã™ã€‚ å°‘ã—æ™‚é–“ã‚’ã‹ã‘ã¦ã‚¯ãƒ¬ãƒ¼ãƒ ã‚’èª­ã‚“ã§ãã ã•ã„ï¼ˆã‚ãªãŸã¯ãã‚Œã‚‰ã®ã„ãã¤ã‹ã‚’ã‹ãªã‚Šã‚³ãƒŸã‚«ãƒ«ã«æ„Ÿã˜ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = './data'\n",
    "base_data_url = 'https://databricksdemostore.blob.core.windows.net/data/05.03/'\n",
    "filesToDownload = ['claims_text.txt', 'claims_labels.txt']\n",
    "\n",
    "os.makedirs(data_location, exist_ok=True)\n",
    "\n",
    "for file in filesToDownload:\n",
    "    data_url = os.path.join(base_data_url, file)\n",
    "    local_file_path = os.path.join(data_location, file)\n",
    "    urllib.request.urlretrieve(data_url, local_file_path)\n",
    "    print('Downloaded file: ', file)\n",
    "    \n",
    "claims_corpus = [claim for claim in open(os.path.join(data_location, 'claims_text.txt'))]\n",
    "claims_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚¯ãƒ¬ãƒ¼ãƒ ã‚µãƒ³ãƒ—ãƒ«ã«åŠ ãˆã¦ã€Contoso Ltdã¯ã€æä¾›ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã‚¯ãƒ¬ãƒ¼ãƒ ã®ãã‚Œãã‚Œã«0ï¼ˆã€Œä½å®…ä¿é™ºã‚¯ãƒ¬ãƒ¼ãƒ ã€ï¼‰ã¾ãŸã¯1ï¼ˆã€Œè‡ªå‹•è»Šä¿é™ºã‚¯ãƒ¬ãƒ¼ãƒ ã€ï¼‰ã®ã„ãšã‚Œã‹ã¨ã—ã¦ãƒ©ãƒ™ãƒ«ã‚’ä»˜ã‘ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚æä¾›ã—ã¦ã„ã¾ã™ã€‚ ã“ã‚Œã¯ã€ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«1è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦æç¤ºã•ã‚Œã€ã‚¯ãƒ¬ãƒ¼ãƒ ãƒ†ã‚­ã‚¹ãƒˆã¨åŒã˜é †åºã§æç¤ºã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€æä¾›ã•ã‚ŒãŸClaims_labels.txtãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª¿ã¹ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [int(re.sub(\"\\n\", \"\", label)) for label in open(os.path.join(data_location, 'claims_labels.txt'))]\n",
    "print(len(labels))\n",
    "print(labels[0:5]) # first 5 labels\n",
    "print(labels[-5:]) # last 5 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šè¨˜ã®å‡ºåŠ›ã‹ã‚‰ã‚ã‹ã‚‹ã‚ˆã†ã«ã€å€¤ã¯æ•´æ•°0ã¾ãŸã¯1ã§ã™ã€‚ã“ã‚Œã‚‰ã‚’ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹ãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ã“ã‚Œã‚‰ã®æ•´æ•°å€¤ã‚’ã‚«ãƒ†ã‚´ãƒªå€¤ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆä»–ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åˆ—æŒ™å‹ã®ã‚ˆã†ãªã‚‚ã®ã¨è€ƒãˆã¦ãã ã•ã„ï¼‰ è¨€èªï¼‰ã€‚\n",
    "\n",
    "`keras.utils` ã® to_categorical ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ã“ã‚Œã‚‰ã®å€¤ã‚’ãƒã‚¤ãƒŠãƒªã®ã‚«ãƒ†ã‚´ãƒªå€¤ã«å¤‰æ›ã§ãã¾ã™ã€‚ æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(labels, 2)\n",
    "print(labels.shape)\n",
    "print()\n",
    "print(labels[0:2]) # first 2 categorical labels\n",
    "print()\n",
    "print(labels[-2:]) # last 2 categorical labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚¯ãƒ¬ãƒ¼ãƒ ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ©ãƒ™ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚ŒãŸã®ã§ã€ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ—ãƒ­ã‚»ã‚¹ã®æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã€ã¤ã¾ã‚Šãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ã™ã‚‹æº–å‚™ãŒã§ãã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ã‚¯ãƒ¬ãƒ¼ãƒ ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’å‡¦ç†ã™ã‚‹\n",
    "\n",
    "- ã™ã¹ã¦ã®å˜èªã‚’å°æ–‡å­—ã«ã—ã¾ã™\n",
    "- åç¸®ã‚’æ‹¡å¼µã™ã‚‹ï¼ˆãŸã¨ãˆã°ã€ \"can't\" ãŒ \"cannot\" ã«ãªã‚‹ï¼‰\n",
    "- ç‰¹æ®Šæ–‡å­—ï¼ˆå¥èª­ç‚¹ãªã©ï¼‰ã‚’å‰Šé™¤ã™ã‚‹\n",
    "- ã‚¯ãƒ¬ãƒ¼ãƒ ãƒ†ã‚­ã‚¹ãƒˆå†…ã®å˜èªã®ãƒªã‚¹ãƒˆã‚’ã€è¾æ›¸å†…ã®ãã‚Œã‚‰ã®å˜èªã®å¯¾å¿œã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚ æ›¸é¢ã®ã‚¯ãƒ¬ãƒ¼ãƒ ã«è¡¨ç¤ºã•ã‚Œã‚‹å˜èªã®é †åºã¯ç¶­æŒã•ã‚Œã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€ã‚¯ãƒ¬ãƒ¼ãƒ ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’å‡¦ç†ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(token):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_token = pattern.sub('', token)\n",
    "    return filtered_token\n",
    "\n",
    "def convert_to_indices(corpus, dictionary, c_map, unk_word_index = 399999):\n",
    "    sequences = []\n",
    "    for i in range(len(corpus)):\n",
    "        tokens = corpus[i].split()\n",
    "        sequence = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word in c_map:\n",
    "                resolved_words = c_map[word].split()\n",
    "                for resolved_word in resolved_words:\n",
    "                    try:\n",
    "                        word_index = dictionary.index(resolved_word)\n",
    "                        sequence.append(word_index)\n",
    "                    except ValueError:\n",
    "                        sequence.append(unk_word_index) #Vector for unkown words\n",
    "            else:\n",
    "                try:\n",
    "                    clean_word = remove_special_characters(word)\n",
    "                    if len(clean_word) > 0:\n",
    "                        word_index = dictionary.index(clean_word)\n",
    "                        sequence.append(word_index)\n",
    "                except ValueError:\n",
    "                    sequence.append(unk_word_index) #Vector for unkown words\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "claims_corpus_indices = convert_to_indices(claims_corpus, dictionary, contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚¯ãƒ¬ãƒ¼ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç¢ºèªã™ã‚‹**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_special_characters(claims_corpus[5]).split())\n",
    "print()\n",
    "print('Ordered list of indices for the above claim')\n",
    "print(claims_corpus_indices[5])\n",
    "print('')\n",
    "print('For example, the index of second word in the claims text \\\"pedestrian\\\" is: ', dictionary.index('pedestrian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å›ºå®šé•·ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆã™ã‚‹**\n",
    "\n",
    "ã‚¯ãƒ¬ãƒ¼ãƒ ã§ä½¿ç”¨ã•ã‚Œã‚‹å˜èªã®æ•°ã¯ã€ã‚¯ãƒ¬ãƒ¼ãƒ ã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™ã€‚ å›ºå®šã‚µã‚¤ã‚ºã®å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ `keras.preprocessing.sequence` ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ` pad_sequences` ã‚’ä½¿ç”¨ã—ã¦ã€å˜èªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å›ºå®šã‚µã‚¤ã‚ºã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆã‚µã‚¤ã‚º= 125ï¼‰ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 125\n",
    "\n",
    "X = pad_sequences(claims_corpus_indices, maxlen=maxSeqLength, padding='pre', truncating='post')\n",
    "\n",
    "print('Review the new fixed size vector for a sample claim')\n",
    "print(remove_special_characters(claims_corpus[5]).split())\n",
    "print()\n",
    "print(X[5])\n",
    "print('')\n",
    "print('Lenght of the vector: ', len(X[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ blob ã‚¹ãƒˆã‚¢ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    "\n",
    "**ä¿å­˜ã•ã‚ŒãŸè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ `Workspace` ã‚’ä½œæˆã—ã¾ã™ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(azureml.core.VERSION)\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®blob ã‚¹ãƒˆã‚¢ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_location = './inputs'\n",
    "features_file = os.path.join(input_location, 'features.npy')\n",
    "labels_file = os.path.join(input_location, 'labels.npy')\n",
    "os.makedirs(input_location, exist_ok=True)\n",
    "np.save(features_file, X)\n",
    "np.save(labels_file, labels)\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(input_location, \n",
    "                 target_path = 'inputs', \n",
    "                 overwrite = True, \n",
    "                 show_progress = True)\n",
    "\n",
    "# Create a file dataset object that represents the features and the labels files\n",
    "dataset = Dataset.File.from_files((datastore, 'inputs/*.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure ML Computeã‚’ä½¿ã£ã¦ãƒªãƒ¢ãƒ¼ãƒˆã§DNNã‚’è¨“ç·´ã™ã‚‹\n",
    "\n",
    "### AML Compute Clusterã®ä½œæˆ\n",
    "\n",
    "Azure Machine Learning Computeã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®Azureä»®æƒ³ãƒã‚·ãƒ³ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ãƒ—ãƒ­ãƒ“ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã—ã¦ç®¡ç†ã™ã‚‹ãŸã‚ã®ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚ç¾åœ¨ã®ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã«æ–°ã—ã„Aml Computeã‚’ä½œæˆã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼ˆã¾ã å­˜åœ¨ã—ã¦ã„ãªã„å ´åˆï¼‰ã€‚ã“ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¸Šã§ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ä½œæˆã«ã¯æ•°åˆ†ã‹ã‹ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create AML CPU based Compute Cluster\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"amlcompute-ad\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC6',\n",
    "                                                           min_nodes=1, max_nodes=1)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current AmlCompute. \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ\n",
    "\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€DNNã‚’æ§‹ç¯‰ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹é€ ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã”è¦§ã®ã‚ˆã†ã«ã€ã“ã®ã‚±ãƒ¼ã‚¹ã§ã¯LSTMãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ã“ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¯ã€å˜èªã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ GloVe å˜èªãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹å˜èªåŸ‹ã‚è¾¼ã¿å±¤ãŒã‚ã‚Šã¾ã™ã€‚æ¬¡ã«ã€GloVeå˜èªãƒ™ã‚¯ãƒˆãƒ«ã¯LSTMå±¤ã«æ¸¡ã•ã‚Œã€ç¶šã„ã¦ãƒã‚¤ãƒŠãƒªåˆ†é¡å™¨å‡ºåŠ›å±¤ã«æ¸¡ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "æ¬¡ã®2ã¤ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã€`scripts`ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_file_folder = './scripts'\n",
    "script_file_name = 'train.py'\n",
    "script_file_full_path = os.path.join(script_file_folder, script_file_name)\n",
    "os.makedirs(script_file_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_file_full_path\n",
    "import os\n",
    "import argparse\n",
    "import urllib.request\n",
    "import math\n",
    "import timeit\n",
    "import numpy as np\n",
    "np.random.seed(437)\n",
    "\n",
    "print(\"numpy version: {}\".format(np.__version__))\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Model as KModel #resolve name conflict with Azure Model\n",
    "from keras import layers, optimizers, regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from azureml.core import Run\n",
    "from azureml.core.model import Model\n",
    "\n",
    "print(\"keras version: {} tensorflow version: {} sklearn version: {}\".format(keras.__version__, \n",
    "                                                                            tensorflow.__version__, \n",
    "                                                                            sklearn.__version__))\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "parser.add_argument(\"--input-folder\", type=str, dest='input_folder', help=\"Reference to inputs\")\n",
    "parser.add_argument('--batch-size', type=int, dest='batch_size', default=16, help='mini batch size for training')\n",
    "parser.add_argument('--lr', type=float, dest='lr', default=0.0005, help='Learning rate')\n",
    "parser.add_argument('--epochs', type=int, dest='epochs', default=25, help='Epochs')\n",
    "\n",
    "args = parser.parse_args()\n",
    "input_location = args.input_folder\n",
    "batch_size = args.batch_size\n",
    "lr = args.lr\n",
    "epochs = args.epochs\n",
    "\n",
    "print('Input folder', input_location)\n",
    "print('Learning rate', lr)\n",
    "print('Batch size', batch_size)\n",
    "print('Epochs', epochs)\n",
    "\n",
    "words_list_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                  'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordsList.npy')\n",
    "word_vectors_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordVectors.npy')\n",
    "word_vectors_dir = './word_vectors'\n",
    "os.makedirs(word_vectors_dir, exist_ok=True)\n",
    "urllib.request.urlretrieve(words_list_url, os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "urllib.request.urlretrieve(word_vectors_url, os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "dictionary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "dictionary = dictionary.tolist()\n",
    "dictionary = [word.decode('UTF-8') for word in dictionary]\n",
    "print('Loaded the dictionary! Dictionary size: ', len(dictionary))\n",
    "word_vectors = np.load(os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "print ('Loaded the word vectors! Shape of the word vectors: ', word_vectors.shape)\n",
    "\n",
    "maxSeqLength = 125\n",
    "embedding_layer = Embedding(word_vectors.shape[0],\n",
    "                            word_vectors.shape[1],\n",
    "                            weights=[word_vectors],\n",
    "                            input_length=maxSeqLength,\n",
    "                            trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "print('Reading input files...')\n",
    "features_file = os.path.join(input_location, 'features.npy')\n",
    "labels_file = os.path.join(input_location, 'labels.npy')\n",
    "X = np.load(features_file)\n",
    "labels = np.load(labels_file)\n",
    "print('Done reading input files.')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=lr)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "run = Run.get_context()\n",
    "class LogRunMetrics(Callback):\n",
    "    # callback at the end of every epoch\n",
    "    def on_epoch_end(self, epoch, log):\n",
    "        # log a value repeated which creates a list\n",
    "        run.log('Loss', log['val_loss'])\n",
    "        run.log('Accuracy', log['val_accuracy'])\n",
    "_callbacks = [LogRunMetrics()]\n",
    "\n",
    "print(\"Model training starting...\")\n",
    "start_time = timeit.default_timer()\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=_callbacks,\n",
    "                    verbose=2)\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(\"Model training completed.\")\n",
    "print('Elapsed time (min): ', round(elapsed_time/60.0,0))\n",
    "\n",
    "# save the model\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "model.save(os.path.join('./outputs', 'final_model.hdf5'))\n",
    "# save training history\n",
    "with open(os.path.join('./outputs', 'history.txt'), 'w') as f:\n",
    "    f.write(str(history.history))\n",
    "\n",
    "print(\"Model saved in ./outputs folder\")\n",
    "print(\"Saving model files completed.\")\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras estimator ã‚’ä½œæˆã—ã¾ã™ã€‚\n",
    "\n",
    "å¼•æ•° **script_params** ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«æ¸¡ã™ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è¾æ›¸ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã“ã®å ´åˆã€ä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«æ¸¡ã—ã¾ã™ã€‚\n",
    "\n",
    "- **input-folder**: ãƒªãƒ¢ãƒ¼ãƒˆã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆã«ãƒã‚¦ãƒ³ãƒˆã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¸ã®ãƒ‘ã‚¹\n",
    "- **lr**: ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®å­¦ç¿’ç‡\n",
    "- **batch-size**: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "- **epochs**: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒãƒƒã‚¯æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    # to mount files referenced by the dataset\n",
    "    '--input-folder': dataset.as_named_input('inputs').as_mount(),\n",
    "    '--lr': 0.001,\n",
    "    '--batch-size': 16,\n",
    "    '--epochs': 100,\n",
    "}\n",
    "\n",
    "keras_est = TensorFlow(source_directory=script_file_folder,\n",
    "                       compute_target=compute_target,\n",
    "                       entry_script=script_file_name, \n",
    "                       script_params=script_params, \n",
    "                       conda_packages=['numpy==1.18.5', 'scikit-learn==0.23.1'], \n",
    "                       pip_packages=['keras==2.3.1'], \n",
    "                       framework_version='2.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ãƒ³ã®é€ä¿¡\n",
    "\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œã‚’AzureMachineLearningã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«é€ä¿¡ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯å¸¸ã«æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚\n",
    "\n",
    "- å®Ÿè¡Œã™ã‚‹å®Ÿé¨“ã‚’ä½œæˆã—ã¾ã™ã€‚\n",
    "- å®Ÿé¨“ã‚’é€ä¿¡ã™ã‚‹ã€‚\n",
    "- å®Ÿè¡ŒãŒå®Œäº†ã™ã‚‹ã®ã‚’å¾…ã¤ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'claims-classification-exp'\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "run = experiment.submit(keras_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç›£è¦–\n",
    "\n",
    "azureml Jupyter ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œã‚’ç›£è¦–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®é€²è¡Œã«åˆã‚ã›ã¦ã€æ¤œè¨¼ç²¾åº¦ã¨æ¤œè¨¼æå¤±ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç›£è¦–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ç´„2-4åˆ†ã§å®Œäº†ã—ã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€**Download the trained models** ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyterã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã® `Output Logs` ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã§ **azureml-logs/70_driver_log.txt** ã‚’é¸æŠã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å‡ºåŠ›ã‚’ç¢ºèªã—ã¾ã™ã€‚\"val_accuracy\" ã¨ã„ã†å€¤ã®æœ€çµ‚å‡ºåŠ›ã‚’è¦‹ã¦ãã ã•ã„ã€‚ã“ã‚Œã¯æ¤œè¨¼ã‚»ãƒƒãƒˆã®ç²¾åº¦ã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒ£ãƒ³ã‚¹ã®ç²¾åº¦ãŒ50%ã¨è€ƒãˆã‚Œã°ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚ˆã‚Šã‚‚è‰¯ã„ãƒ¢ãƒ‡ãƒ«ãªã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n",
    "\n",
    "ã“ã®æ™‚ç‚¹ã§ã¯ãƒ©ãƒ³ãƒ€ãƒ ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ãªãã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚ã“ã‚Œã¯ã‚ãªãŸã®æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã«éãã¾ã›ã‚“! å…¸å‹çš„ãªãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã®ã‚ˆã†ãªã•ã¾ã–ã¾ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç¹°ã‚Šè¿”ã—ã¦ã„ãã¾ã™ã€‚\n",
    "- å­¦ç¿’ã®ãŸã‚ã«ã€ã‚ˆã‚Šå¤šãã®ãƒ©ãƒ™ãƒ«ä»˜ãæ–‡æ›¸ã‚’å–å¾—ã™ã‚‹\n",
    "- éå­¦ç¿’ã‚’é˜²ããŸã‚ã®æ­£å‰‡åŒ–\n",
    "- ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã®ãƒãƒ¼ãƒ‰æ•°ã€å­¦ç¿’ç‡ãªã©ã®ãƒ¢ãƒ‡ãƒ«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ãƒ¢ãƒ‡ãƒ«å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_outputs = './outputs'\n",
    "os.makedirs(local_outputs, exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    if f.startswith('outputs'):\n",
    "        output_file_path = os.path.join(local_outputs, f.split('/')[-1])\n",
    "        print('Downloading from {} to {} ...'.format(f, output_file_path))\n",
    "        run.download_file(name=f, output_file_path=output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åˆ†é¡ã‚¯ãƒ¬ãƒ¼ãƒ ã®ãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ãŸã®ã§ã€ä¸€é€£ã®ã‚¯ãƒ¬ãƒ¼ãƒ ã«å¯¾ã—ã¦è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(os.path.join(local_outputs, 'final_model.hdf5'))\n",
    "print('Model loaded!')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¾ã™ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_claim = ['I crashed my car into a pole.', \n",
    "              'The flood ruined my house.', \n",
    "              'I lost control of my car and fell in the river.']\n",
    "\n",
    "test_claim_indices = convert_to_indices(test_claim, dictionary, contractions)\n",
    "test_data = pad_sequences(test_claim_indices, maxlen=maxSeqLength, padding='pre', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦åˆ†é¡ã‚’äºˆæ¸¬ã™ã‚‹**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)\n",
    "pred_label = pred.argmax(axis=1)\n",
    "pred_df = pd.DataFrame(np.column_stack((pred,pred_label)), columns=['class_0', 'class_1', 'label'])\n",
    "pred_df.label = pred_df.label.astype(int)\n",
    "print('Predictions')\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€å¾Œã«ã€ã‚‚ã£ã¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¤ã„ã¦è§¦ã‚ŠãŸã„ã¨ã„ã†æ–¹ã®ãŸã‚ã«ã€è¿½åŠ ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”¨æ„ã—ã¦ã‚ã‚Šã¾ã™ã€‚\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚„å­¦ç¿’æ™‚é–“ã€æ¨è«–é€Ÿåº¦ã®æ¯”è¼ƒã‚’ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚Kerasã§ç°¡å˜ã«è©¦ã™ã“ã¨ãŒã§ãã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã—ã¦ã€LSTM ã®ã»ã‹ã« SimpleRNN ã¨ GRU ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "[SimpleRNN/GRU/LSTMã®æ¯”è¼ƒ](./06%20Compare%20deep%20learning%20algorithms.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
