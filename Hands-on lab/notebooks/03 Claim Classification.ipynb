{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kerasによるクレーム分類：Pythonディープラーニングライブラリ\n",
    "\n",
    "このノートブックでは、請求が自動車保険請求である場合は `1` 、住宅保険請求である場合は `0` を予測する請求テキストの分類モデルをトレーニングします。 モデルは、Kerasライブラリを介してTensorFlowを使用し、Long Short-Term Memory（LSTM）リカレントニューラルネットワークと呼ばれるDNNのタイプを使用して構築されます。\n",
    "\n",
    "このノートブックでは、以下で構成されるテキスト分析プロセスについて説明します。\n",
    "\n",
    "- GloVe単語埋め込みと単語アナロジーの例\n",
    "- GloVe単語の埋め込みを使用したトレーニングデータのベクトル化\n",
    "- LSTMベースの分類モデルの作成とトレーニング\n",
    "- トレーニング指標をリアルタイムで監視する\n",
    "- モデルを使用して分類を予測する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モジュールを準備する\n",
    "\n",
    "このノートブックは、Kerasライブラリを使用して分類器を作成およびトレーニングします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print('Keras version: ', keras.__version__)\n",
    "print('Tensorflow version: ', tf.__version__)\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Experiment, Workspace, Run, Datastore\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.model import Model\n",
    "from azureml.train.dnn import TensorFlow\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "print(\"Azure ML SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**事前トレーニング済みのGloVe単語の埋め込みをダウンロードして、このノートブックにロードしてみましょう。**\n",
    "\n",
    "これで、サイズが **400,000** ワードの `dictionary`と、辞書内の単語に対応する ` GloVe wordベクトル ` が作成されます。 各単語ベクトルのサイズは50であるため、ここで使用される単語埋め込みの次元は **50** です。\n",
    "\n",
    "*次のセルの実行には数分かかる場合があります*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                  'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordsList.npy')\n",
    "\n",
    "word_vectors_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordVectors.npy')\n",
    "\n",
    "word_vectors_dir = './word_vectors'\n",
    "\n",
    "os.makedirs(word_vectors_dir, exist_ok=True)\n",
    "urllib.request.urlretrieve(words_list_url, os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "urllib.request.urlretrieve(word_vectors_url, os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "\n",
    "dictionary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "dictionary = dictionary.tolist()\n",
    "dictionary = [word.decode('UTF-8') for word in dictionary]\n",
    "print('Loaded the dictionary! Dictionary size: ', len(dictionary))\n",
    "\n",
    "word_vectors = np.load(os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "print ('Loaded the word vectors! Shape of the word vectors: ', word_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**収縮マップという単語を作成します。 マップはコーパスの縮約を拡張するために使用されます（たとえば、 \"can't\" は\"cannot\"になります）。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/contractions.xlsx')\n",
    "contractions_df = pd.read_excel(contractions_url)\n",
    "contractions = dict(zip(contractions_df.original, contractions_df.expanded))\n",
    "print('Review first 10 entries from the contractions map')\n",
    "print(contractions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe単語埋め込みを使用したワードアナロジーの例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVeは、辞書内の各単語をベクトルとして表します。ワードアナロジーを予測するために単語ベクトルを使用できます。\n",
    "\n",
    "次の類推を解決する以下の例を参照してください。 **man と woman の関係は、king と\"何\"の関係に対応するか？**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コサイン類似度は、2つの単語の類似度を評価するために使用される尺度です。 このヘルパー関数は、2つの単語のベクトルを取り、-1〜1の範囲のコサイン類似度を返します。同義語の場合、コサイン類似度は1に近く、反意語の場合、コサイン類似度は-1に近くなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    dot = u.dot(v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    cosine_similarity = dot/norm_u/norm_v\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語のベクトルを確認してみましょう **man**, **woman**, and **king**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man = word_vectors[dictionary.index('man')]\n",
    "woman = word_vectors[dictionary.index('woman')]\n",
    "king = word_vectors[dictionary.index('king')]\n",
    "print(man)\n",
    "print('')\n",
    "print(woman)\n",
    "print('')\n",
    "print(king)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "類推を解くには、次の方程式でxを解く必要があります。\n",
    "\n",
    "**woman – man = x - king**\n",
    "\n",
    "したがって, **x = woman - man + king**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = woman - man + king"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**次に、単語ベクトルが上記で計算されたベクトルxに最も近い単語を見つけます。**\n",
    "\n",
    "計算コストを制限するために、辞書全体を検索するのではなく、可能な回答のリストから最適な単語を特定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = ['woman', 'prince', 'princess', 'england', 'goddess', 'diva', 'empress', \n",
    "           'female', 'lady', 'monarch', 'title', 'queen', 'sovereign', 'ruler', \n",
    "           'male', 'crown', 'majesty', 'royal', 'cleopatra', 'elizabeth', 'victoria', \n",
    "           'throne', 'internet', 'sky', 'machine', 'learning', 'fairy']\n",
    "\n",
    "df = pd.DataFrame(columns = ['word', 'cosine_similarity'])\n",
    "\n",
    "# Find the similarity of each word in answers with x\n",
    "for w in answers:\n",
    "    sim = cosine_similarity(word_vectors[dictionary.index(w)], x)   \n",
    "    df = df.append({'word': w, 'cosine_similarity': sim}, ignore_index=True)\n",
    "    \n",
    "df.sort_values(['cosine_similarity'], ascending=False, inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**上記の結果から、単語「queen」のベクトルがベクトル「x」に最も類似していることがわかります。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数値ばかりで分かりにくいでしょうか？\n",
    "\n",
    "**【補講】**\n",
    "単語ベクトル表現のさらなる理解のために、ベクトルを可視化してみましょう。\n",
    "多次元ベクトルを可視化するためには、次元削減という手法を用います。PCA(主成分分析)を使用すると、元のベクトルが持つ情報をなるべく欠損することなく縮約することができます。今回は50次元のデータを2次元に減らします。これを実行すれば、matplotlib散布図を使用して2次元グラフ上にプロットすることができるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトル同士の演算\n",
    "man = word_vectors[dictionary.index('man')]\n",
    "woman = word_vectors[dictionary.index('woman')]\n",
    "king = word_vectors[dictionary.index('king')]\n",
    "x = woman - man + king\n",
    "\n",
    "words = ['king', 'man', 'woman', 'queen']\n",
    "# GloVe 辞書から指定した単語のベクトルを取り出す\n",
    "vectors = [word_vectors[dictionary.index(word)] for word in words]\n",
    "\n",
    "# ベクトル演算結果の確認のため、配列の末尾に演算結果xを追加\n",
    "vectors.append(x)\n",
    "words.append('x')\n",
    "\n",
    "# 50次元を2次元に削減\n",
    "twodim = PCA(n_components=2).fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "# 散布図を作成\n",
    "plt.scatter(twodim[:, 0], twodim[:, 1], edgecolors='k', c='b')\n",
    "\n",
    "for label, x, y in zip(words, twodim[:, 0], twodim[:, 1]):\n",
    "    labeltitle = label + \"(\" + str(np.round(x, decimals=1)) + \",\" + str(np.round(y, decimals=1)) + \")\"\n",
    "    plt.annotate(labeltitle, xy=(x, y), xytext=(5, 0), textcoords=\"offset points\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「王」から「男性」を引いて「女性」を足したとき、それは「女王」となるであろうという類推が、GloVeベクトルの演算で可能になりました。これは何を意味するかというと、単語間の意味的な関係をベクトルで表現することができるということです。この技術の登場によって、単語をニューラルネットワークのインプットにすることができるようになり、機械翻訳や文章の固有表現抽出など幅広い領域に用いられるようになりました。\n",
    "\n",
    "それではもっと面白い特徴をお見せしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトル同士の演算\n",
    "man = word_vectors[dictionary.index('man')]\n",
    "woman = word_vectors[dictionary.index('woman')]\n",
    "king = word_vectors[dictionary.index('king')]\n",
    "x = woman - man + king\n",
    "\n",
    "# GloVe 辞書内に含まれる単語をカテゴリごとに追加（自由に編集してください）\n",
    "words = ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', \n",
    "         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
    "         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
    "         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
    "         'france', 'germany', 'hungary', 'luxembourg', 'australia', 'japan',\n",
    "         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
    "         'school', 'college', 'university', 'institute',\n",
    "         'king', 'man', 'woman', 'queen']\n",
    "\n",
    "# GloVe 辞書から指定した単語のベクトルを取り出す\n",
    "vectors = [word_vectors[dictionary.index(word)] for word in words]\n",
    "\n",
    "# ベクトル演算結果の確認のため、配列の末尾に演算結果xを追加\n",
    "vectors.append(x)\n",
    "words.append('x')\n",
    "\n",
    "# 50次元を2次元に削減\n",
    "twodim = PCA(n_components=2).fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "# 散布図を作成\n",
    "plt.scatter(twodim[:, 0], twodim[:, 1], edgecolors='k', c='b')\n",
    "\n",
    "for label, x, y in zip(words, twodim[:, 0], twodim[:, 1]):\n",
    "    labeltitle = label + \"(\" + str(np.round(x, decimals=1)) + \",\" + str(np.round(y, decimals=1)) + \")\"\n",
    "    plt.annotate(labeltitle, xy=(x, y), xytext=(5, 0), textcoords=\"offset points\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "似た意味の単語同士がベクトル空間上で近い位置に配置されることがグラフから分かりましたか？これは魔法のようなことです！🧙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレーニングデータを準備する\n",
    "\n",
    "Contoso Ltdは、クレームテキストとして受け取るテキストの例を含む小さなドキュメントを提供しています。 彼らは、これをサンプルクレームごとに1行のテキストファイルで提供しています。\n",
    "\n",
    "次のセルを実行して、ファイルの内容をダウンロードして確認します。 少し時間をかけてクレームを読んでください（あなたはそれらのいくつかをかなりコミカルに感じるかもしれません！）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = './data'\n",
    "base_data_url = 'https://databricksdemostore.blob.core.windows.net/data/05.03/'\n",
    "filesToDownload = ['claims_text.txt', 'claims_labels.txt']\n",
    "\n",
    "os.makedirs(data_location, exist_ok=True)\n",
    "\n",
    "for file in filesToDownload:\n",
    "    data_url = os.path.join(base_data_url, file)\n",
    "    local_file_path = os.path.join(data_location, file)\n",
    "    urllib.request.urlretrieve(data_url, local_file_path)\n",
    "    print('Downloaded file: ', file)\n",
    "    \n",
    "claims_corpus = [claim for claim in open(os.path.join(data_location, 'claims_text.txt'))]\n",
    "claims_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クレームサンプルに加えて、Contoso Ltdは、提供されたサンプルクレームのそれぞれに0（「住宅保険クレーム」）または1（「自動車保険クレーム」）のいずれかとしてラベルを付けるドキュメントも提供しています。 これは、サンプルごとに1行のテキストファイルとして提示され、クレームテキストと同じ順序で提示されます。\n",
    "\n",
    "次のセルを実行して、提供されたClaims_labels.txtファイルの内容を調べます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [int(re.sub(\"\\n\", \"\", label)) for label in open(os.path.join(data_location, 'claims_labels.txt'))]\n",
    "print(len(labels))\n",
    "print(labels[0:5]) # first 5 labels\n",
    "print(labels[-5:]) # last 5 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の出力からわかるように、値は整数0または1です。これらをモデルのトレーニングに使用するラベルとして使用するには、これらの整数値をカテゴリ値に変換する必要があります（他のプログラミングの列挙型のようなものと考えてください） 言語）。\n",
    "\n",
    "`keras.utils` の to_categorical メソッドを使用して、これらの値をバイナリのカテゴリ値に変換できます。 次のセルを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(labels, 2)\n",
    "print(labels.shape)\n",
    "print()\n",
    "print(labels[0:2]) # first 2 categorical labels\n",
    "print()\n",
    "print(labels[-2:]) # last 2 categorical labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クレームテキストとラベルが読み込まれたので、テキスト分析プロセスの最初のステップ、つまりテキストを正規化する準備ができました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クレームコーパスを処理する\n",
    "\n",
    "- すべての単語を小文字にします\n",
    "- 収縮を拡張する（たとえば、 \"can't\" が \"cannot\" になる）\n",
    "- 特殊文字（句読点など）を削除する\n",
    "- クレームテキスト内の単語のリストを、辞書内のそれらの単語の対応するインデックスのリストに変換します。 書面のクレームに表示される単語の順序は維持されることに注意してください。\n",
    "\n",
    "次のセルを実行して、クレームコーパスを処理します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(token):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_token = pattern.sub('', token)\n",
    "    return filtered_token\n",
    "\n",
    "def convert_to_indices(corpus, dictionary, c_map, unk_word_index = 399999):\n",
    "    sequences = []\n",
    "    for i in range(len(corpus)):\n",
    "        tokens = corpus[i].split()\n",
    "        sequence = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word in c_map:\n",
    "                resolved_words = c_map[word].split()\n",
    "                for resolved_word in resolved_words:\n",
    "                    try:\n",
    "                        word_index = dictionary.index(resolved_word)\n",
    "                        sequence.append(word_index)\n",
    "                    except ValueError:\n",
    "                        sequence.append(unk_word_index) #Vector for unkown words\n",
    "            else:\n",
    "                try:\n",
    "                    clean_word = remove_special_characters(word)\n",
    "                    if len(clean_word) > 0:\n",
    "                        word_index = dictionary.index(clean_word)\n",
    "                        sequence.append(word_index)\n",
    "                except ValueError:\n",
    "                    sequence.append(unk_word_index) #Vector for unkown words\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "claims_corpus_indices = convert_to_indices(claims_corpus, dictionary, contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1つのサンプルクレームのインデックスを確認する**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_special_characters(claims_corpus[5]).split())\n",
    "print()\n",
    "print('Ordered list of indices for the above claim')\n",
    "print(claims_corpus_indices[5])\n",
    "print('')\n",
    "print('For example, the index of second word in the claims text \\\"pedestrian\\\" is: ', dictionary.index('pedestrian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**固定長ベクトルを作成する**\n",
    "\n",
    "クレームで使用される単語の数は、クレームによって異なります。 固定サイズの入力ベクトルを作成する必要があります。 `keras.preprocessing.sequence` のユーティリティ関数 ` pad_sequences` を使用して、単語インデックスの固定サイズのベクトル（サイズ= 125）を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 125\n",
    "\n",
    "X = pad_sequences(claims_corpus_indices, maxlen=maxSeqLength, padding='pre', truncating='post')\n",
    "\n",
    "print('Review the new fixed size vector for a sample claim')\n",
    "print(remove_special_characters(claims_corpus[5]).split())\n",
    "print()\n",
    "print(X[5])\n",
    "print('')\n",
    "print('Lenght of the vector: ', len(X[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニングデータを blob ストアにアップロードする\n",
    "\n",
    "**保存された設定ファイルから `Workspace` を作成します。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(azureml.core.VERSION)\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**特徴量とラベルをデフォルトのblob ストアにアップロードする**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_location = './inputs'\n",
    "features_file = os.path.join(input_location, 'features.npy')\n",
    "labels_file = os.path.join(input_location, 'labels.npy')\n",
    "os.makedirs(input_location, exist_ok=True)\n",
    "np.save(features_file, X)\n",
    "np.save(labels_file, labels)\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(input_location, \n",
    "                 target_path = 'inputs', \n",
    "                 overwrite = True, \n",
    "                 show_progress = True)\n",
    "\n",
    "# Create a file dataset object that represents the features and the labels files\n",
    "dataset = Dataset.File.from_files((datastore, 'inputs/*.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure ML Computeを使ってリモートでDNNを訓練する\n",
    "\n",
    "### AML Compute Clusterの作成\n",
    "\n",
    "Azure Machine Learning Computeは、機械学習ワークロードを実行するためのAzure仮想マシンのクラスタをプロビジョニングして管理するためのサービスです。現在のワークスペースに新しいAml Computeを作成してみましょう（まだ存在していない場合）。このコンピュートターゲット上でモデルトレーニングジョブを実行します。作成には数分かかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create AML CPU based Compute Cluster\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"amlcompute-ad\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC6',\n",
    "                                                           min_nodes=1, max_nodes=1)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current AmlCompute. \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニングスクリプトの作成\n",
    "\n",
    "トレーニングスクリプトは、DNNを構築してトレーニングします。ニューラルネットワークの構造を理解するために、以下のコードを確認してください。ご覧のように、このケースではLSTMリカレント・ニューラル・ネットワークを構築します。このネットワークには、単語のインデックスを GloVe 単語ベクトルに変換する単語埋め込み層があります。次に、GloVe単語ベクトルはLSTM層に渡され、続いてバイナリ分類器出力層に渡されます。\n",
    "\n",
    "次の2つのセルを実行して学習スクリプトを作成し、`scripts`フォルダに保存する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_file_folder = './scripts'\n",
    "script_file_name = 'train.py'\n",
    "script_file_full_path = os.path.join(script_file_folder, script_file_name)\n",
    "os.makedirs(script_file_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_file_full_path\n",
    "import os\n",
    "import argparse\n",
    "import urllib.request\n",
    "import math\n",
    "import timeit\n",
    "import numpy as np\n",
    "np.random.seed(437)\n",
    "\n",
    "print(\"numpy version: {}\".format(np.__version__))\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Model as KModel #resolve name conflict with Azure Model\n",
    "from keras import layers, optimizers, regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from azureml.core import Run\n",
    "from azureml.core.model import Model\n",
    "\n",
    "print(\"keras version: {} tensorflow version: {} sklearn version: {}\".format(keras.__version__, \n",
    "                                                                            tensorflow.__version__, \n",
    "                                                                            sklearn.__version__))\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "parser.add_argument(\"--input-folder\", type=str, dest='input_folder', help=\"Reference to inputs\")\n",
    "parser.add_argument('--batch-size', type=int, dest='batch_size', default=16, help='mini batch size for training')\n",
    "parser.add_argument('--lr', type=float, dest='lr', default=0.0005, help='Learning rate')\n",
    "parser.add_argument('--epochs', type=int, dest='epochs', default=25, help='Epochs')\n",
    "\n",
    "args = parser.parse_args()\n",
    "input_location = args.input_folder\n",
    "batch_size = args.batch_size\n",
    "lr = args.lr\n",
    "epochs = args.epochs\n",
    "\n",
    "print('Input folder', input_location)\n",
    "print('Learning rate', lr)\n",
    "print('Batch size', batch_size)\n",
    "print('Epochs', epochs)\n",
    "\n",
    "words_list_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                  'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordsList.npy')\n",
    "word_vectors_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordVectors.npy')\n",
    "word_vectors_dir = './word_vectors'\n",
    "os.makedirs(word_vectors_dir, exist_ok=True)\n",
    "urllib.request.urlretrieve(words_list_url, os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "urllib.request.urlretrieve(word_vectors_url, os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "dictionary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "dictionary = dictionary.tolist()\n",
    "dictionary = [word.decode('UTF-8') for word in dictionary]\n",
    "print('Loaded the dictionary! Dictionary size: ', len(dictionary))\n",
    "word_vectors = np.load(os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "print ('Loaded the word vectors! Shape of the word vectors: ', word_vectors.shape)\n",
    "\n",
    "maxSeqLength = 125\n",
    "embedding_layer = Embedding(word_vectors.shape[0],\n",
    "                            word_vectors.shape[1],\n",
    "                            weights=[word_vectors],\n",
    "                            input_length=maxSeqLength,\n",
    "                            trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "print('Reading input files...')\n",
    "features_file = os.path.join(input_location, 'features.npy')\n",
    "labels_file = os.path.join(input_location, 'labels.npy')\n",
    "X = np.load(features_file)\n",
    "labels = np.load(labels_file)\n",
    "print('Done reading input files.')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=lr)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "run = Run.get_context()\n",
    "class LogRunMetrics(Callback):\n",
    "    # callback at the end of every epoch\n",
    "    def on_epoch_end(self, epoch, log):\n",
    "        # log a value repeated which creates a list\n",
    "        run.log('Loss', log['val_loss'])\n",
    "        run.log('Accuracy', log['val_accuracy'])\n",
    "_callbacks = [LogRunMetrics()]\n",
    "\n",
    "print(\"Model training starting...\")\n",
    "start_time = timeit.default_timer()\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=_callbacks,\n",
    "                    verbose=2)\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(\"Model training completed.\")\n",
    "print('Elapsed time (min): ', round(elapsed_time/60.0,0))\n",
    "\n",
    "# save the model\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "model.save(os.path.join('./outputs', 'final_model.hdf5'))\n",
    "# save training history\n",
    "with open(os.path.join('./outputs', 'history.txt'), 'w') as f:\n",
    "    f.write(str(history.history))\n",
    "\n",
    "print(\"Model saved in ./outputs folder\")\n",
    "print(\"Saving model files completed.\")\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras estimator を作成します。\n",
    "\n",
    "引数 **script_params** は、トレーニングスクリプトに渡すコマンドライン引数の辞書であることに注意してください。この場合、以下のパラメータをトレーニングスクリプトに渡します。\n",
    "\n",
    "- **input-folder**: リモートコンピュートにマウントされたトレーニングデータセットへのパス\n",
    "- **lr**: オプティマイザの学習率\n",
    "- **batch-size**: トレーニング用バッチサイズ\n",
    "- **epochs**: トレーニングエポック数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    # to mount files referenced by the dataset\n",
    "    '--input-folder': dataset.as_named_input('inputs').as_mount(),\n",
    "    '--lr': 0.001,\n",
    "    '--batch-size': 16,\n",
    "    '--epochs': 100,\n",
    "}\n",
    "\n",
    "keras_est = TensorFlow(source_directory=script_file_folder,\n",
    "                       compute_target=compute_target,\n",
    "                       entry_script=script_file_name, \n",
    "                       script_params=script_params, \n",
    "                       conda_packages=['numpy==1.18.5', 'scikit-learn==0.23.1'], \n",
    "                       pip_packages=['keras==2.3.1'], \n",
    "                       framework_version='2.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニングランの送信\n",
    "\n",
    "トレーニング実行をAzureMachineLearningコンピューティングターゲットに送信するためのコードパターンは常に次のとおりです。\n",
    "\n",
    "- 実行する実験を作成します。\n",
    "- 実験を送信する。\n",
    "- 実行が完了するのを待つ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'claims-classification-exp'\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "run = experiment.submit(keras_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行メトリクスの監視\n",
    "\n",
    "azureml Jupyter ウィジェットを使用して、トレーニング実行を監視することができます。トレーニングの進行に合わせて、検証精度と検証損失をリアルタイムで監視することができます。\n",
    "\n",
    "トレーニングは約2-4分で完了します。トレーニングが完了したら、**Download the trained models** セルを実行して、トレーニングされたモデルをローカルにダウンロードすることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyterウィジェットの `Output Logs` ドロップダウンで **azureml-logs/70_driver_log.txt** を選択して、モデルトレーニングの出力を確認します。\"val_accuracy\" という値の最終出力を見てください。これは検証セットの精度を表しています。ランダムチャンスの精度が50%と考えれば、ランダムよりも良いモデルなのでしょうか？\n",
    "\n",
    "この時点ではランダムよりも優れていなくても構いません。これはあなたの最初のモデルに過ぎません! 典型的なデータサイエンスのプロセスでは、モデルの精度を向上させるために、以下のようなさまざまなアクションを繰り返していきます。\n",
    "- 学習のために、より多くのラベル付き文書を取得する\n",
    "- 過学習を防ぐための正則化\n",
    "- レイヤー数、レイヤーごとのノード数、学習率などのモデルハイパーパラメータの調整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習したモデルとモデル履歴ファイルをダウンロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_outputs = './outputs'\n",
    "os.makedirs(local_outputs, exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    if f.startswith('outputs'):\n",
    "        output_file_path = os.path.join(local_outputs, f.split('/')[-1])\n",
    "        print('Downloading from {} to {} ...'.format(f, output_file_path))\n",
    "        run.download_file(name=f, output_file_path=output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類クレームのテスト\n",
    "\n",
    "モデルを作成したので、一連のクレームに対して試してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**訓練されたモデルをロードします**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(os.path.join(local_outputs, 'final_model.hdf5'))\n",
    "print('Model loaded!')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**以下のセルを実行して、テストデータを準備します。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_claim = ['I crashed my car into a pole.', \n",
    "              'The flood ruined my house.', \n",
    "              'I lost control of my car and fell in the river.']\n",
    "\n",
    "test_claim_indices = convert_to_indices(test_claim, dictionary, contractions)\n",
    "test_data = pad_sequences(test_claim_indices, maxlen=maxSeqLength, padding='pre', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**モデルを使って分類を予測する**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)\n",
    "pred_label = pred.argmax(axis=1)\n",
    "pred_df = pd.DataFrame(np.column_stack((pred,pred_label)), columns=['class_0', 'class_1', 'label'])\n",
    "pred_df.label = pred_df.label.astype(int)\n",
    "print('Predictions')\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、もっとディープラーニングについて触りたいという方のために、追加のコンテンツを用意してあります。\n",
    "このノートブックでは、複数のテキスト分類アルゴリズムを使ってモデルの精度や学習時間、推論速度の比較をすることができます。Kerasで簡単に試すことができるアルゴリズムとして、LSTM のほかに SimpleRNN と GRU があります。\n",
    "\n",
    "[SimpleRNN/GRU/LSTMの比較](./06%20Compare%20deep%20learning%20algorithms.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
