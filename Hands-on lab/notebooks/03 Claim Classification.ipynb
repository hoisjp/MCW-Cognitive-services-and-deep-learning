{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kerasã«ã‚ˆã‚‹ã‚¯ãƒ¬ãƒ¼ãƒ åˆ†é¡ï¼šPythonãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€è«‹æ±‚ãŒè‡ªå‹•è»Šä¿é™ºè«‹æ±‚ã§ã‚ã‚‹å ´åˆã¯ `1` ã€ä½å®…ä¿é™ºè«‹æ±‚ã§ã‚ã‚‹å ´åˆã¯ `0` ã‚’äºˆæ¸¬ã™ã‚‹è«‹æ±‚ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚ ãƒ¢ãƒ‡ãƒ«ã¯ã€Kerasãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä»‹ã—ã¦TensorFlowã‚’ä½¿ç”¨ã—ã€Long Short-Term Memoryï¼ˆLSTMï¼‰ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨å‘¼ã°ã‚Œã‚‹DNNã®ã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨ã—ã¦æ§‹ç¯‰ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ä»¥ä¸‹ã§æ§‹æˆã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚\n",
    "\n",
    "- GloVeå˜èªã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã—ãŸä¾‹ã®é¡æ¨\n",
    "- GloVeå˜èªã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–\n",
    "- LSTMãƒ™ãƒ¼ã‚¹ã®åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "- ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦åˆ†é¡ã‚’äºˆæ¸¬ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æº–å‚™ã™ã‚‹\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Kerasãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦åˆ†é¡å™¨ã‚’ä½œæˆãŠã‚ˆã³ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print('Keras version: ', keras.__version__)\n",
    "print('Tensorflow version: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®GloVeå˜èªã®åŸ‹ã‚è¾¼ã¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«ãƒ­ãƒ¼ãƒ‰ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚**\n",
    "\n",
    "ã“ã‚Œã§ã€ã‚µã‚¤ã‚ºãŒ **400,000** ãƒ¯ãƒ¼ãƒ‰ã® `dictionary`ã¨ã€è¾æ›¸å†…ã®å˜èªã«å¯¾å¿œã™ã‚‹ ` GloVe wordãƒ™ã‚¯ãƒˆãƒ« ` ãŒä½œæˆã•ã‚Œã¾ã™ã€‚ å„å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®ã‚µã‚¤ã‚ºã¯50ã§ã‚ã‚‹ãŸã‚ã€ã“ã“ã§ä½¿ç”¨ã•ã‚Œã‚‹å˜èªåŸ‹ã‚è¾¼ã¿ã®æ¬¡å…ƒã¯ **50** ã§ã™ã€‚\n",
    "\n",
    "*æ¬¡ã®ã‚»ãƒ«ã®å®Ÿè¡Œã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                  'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordsList.npy')\n",
    "\n",
    "word_vectors_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordVectors.npy')\n",
    "\n",
    "word_vectors_dir = './word_vectors'\n",
    "\n",
    "os.makedirs(word_vectors_dir, exist_ok=True)\n",
    "urllib.request.urlretrieve(words_list_url, os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "urllib.request.urlretrieve(word_vectors_url, os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "\n",
    "dictionary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "dictionary = dictionary.tolist()\n",
    "dictionary = [word.decode('UTF-8') for word in dictionary]\n",
    "print('Loaded the dictionary! Dictionary size: ', len(dictionary))\n",
    "\n",
    "word_vectors = np.load(os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "print ('Loaded the word vectors! Shape of the word vectors: ', word_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**åç¸®ãƒãƒƒãƒ—ã¨ã„ã†å˜èªã‚’ä½œæˆã—ã¾ã™ã€‚ ãƒãƒƒãƒ—ã¯ã‚³ãƒ¼ãƒ‘ã‚¹ã®ç¸®ç´„ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ï¼ˆãŸã¨ãˆã°ã€ \"can't\" ã¯\"cannot\"ã«ãªã‚Šã¾ã™ï¼‰ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/contractions.xlsx')\n",
    "contractions_df = pd.read_excel(contractions_url)\n",
    "contractions = dict(zip(contractions_df.original, contractions_df.expanded))\n",
    "print('Review first 10 entries from the contractions map')\n",
    "print(contractions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVeå˜èªåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã—ãŸãƒ¯ãƒ¼ãƒ‰ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼ã®ä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVeã¯ã€è¾æ›¸å†…ã®å„å˜èªã‚’ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦è¡¨ã—ã¾ã™ã€‚ å˜èªã®é¡æ¨ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã«å˜èªãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚\n",
    "\n",
    "æ¬¡ã®é¡æ¨ã‚’è§£æ±ºã™ã‚‹ä»¥ä¸‹ã®ä¾‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ **man ã¨ woman ã®é–¢ä¿‚ã¯ã€king ã¨\"ä½•\"ã®é–¢ä¿‚ã«å¯¾å¿œã™ã‚‹ã‹ï¼Ÿ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã¯ã€2ã¤ã®å˜èªã®é¡ä¼¼åº¦ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹å°ºåº¦ã§ã™ã€‚ ã“ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã¯ã€2ã¤ã®å˜èªã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–ã‚Šã€-1ã€œ1ã®ç¯„å›²ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¿”ã—ã¾ã™ã€‚åŒç¾©èªã®å ´åˆã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã¯1ã«è¿‘ãã€åæ„èªã®å ´åˆã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã¯-1ã«è¿‘ããªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    dot = u.dot(v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    cosine_similarity = dot/norm_u/norm_v\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å˜èªã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã† **man**, **woman**, and **king**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man = word_vectors[dictionary.index('man')]\n",
    "woman = word_vectors[dictionary.index('woman')]\n",
    "king = word_vectors[dictionary.index('king')]\n",
    "print(man)\n",
    "print('')\n",
    "print(woman)\n",
    "print('')\n",
    "print(king)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¡æ¨ã‚’è§£ãã«ã¯ã€æ¬¡ã®æ–¹ç¨‹å¼ã§xã‚’è§£ãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "**woman â€“ man = x - king**\n",
    "\n",
    "ã—ãŸãŒã£ã¦, **x = woman - man + king**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = woman - man + king"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ¬¡ã«ã€å˜èªãƒ™ã‚¯ãƒˆãƒ«ãŒä¸Šè¨˜ã§è¨ˆç®—ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«xã«æœ€ã‚‚è¿‘ã„å˜èªã‚’è¦‹ã¤ã‘ã¾ã™ã€‚**\n",
    "\n",
    "è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’åˆ¶é™ã™ã‚‹ãŸã‚ã«ã€è¾æ›¸å…¨ä½“ã‚’æ¤œç´¢ã™ã‚‹ã®ã§ã¯ãªãã€å¯èƒ½ãªå›ç­”ã®ãƒªã‚¹ãƒˆã‹ã‚‰æœ€é©ãªå˜èªã‚’ç‰¹å®šã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = ['woman', 'prince', 'princess', 'england', 'goddess', 'diva', 'empress', \n",
    "           'female', 'lady', 'monarch', 'title', 'queen', 'sovereign', 'ruler', \n",
    "           'male', 'crown', 'majesty', 'royal', 'cleopatra', 'elizabeth', 'victoria', \n",
    "           'throne', 'internet', 'sky', 'machine', 'learning', 'fairy']\n",
    "\n",
    "df = pd.DataFrame(columns = ['word', 'cosine_similarity'])\n",
    "\n",
    "# Find the similarity of each word in answers with x\n",
    "for w in answers:\n",
    "    sim = cosine_similarity(word_vectors[dictionary.index(w)], x)   \n",
    "    df = df.append({'word': w, 'cosine_similarity': sim}, ignore_index=True)\n",
    "    \n",
    "df.sort_values(['cosine_similarity'], ascending=False, inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ä¸Šè¨˜ã®çµæœã‹ã‚‰ã€å˜èªã€Œqueenã€ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒãƒ™ã‚¯ãƒˆãƒ«ã€Œxã€ã«æœ€ã‚‚é¡ä¼¼ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ•°å€¤ã°ã‹ã‚Šã§åˆ†ã‹ã‚Šã«ãã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n",
    "\n",
    "**ã€è£œè¬›ã€‘**\n",
    "å˜èªãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã®ã•ã‚‰ãªã‚‹ç†è§£ã®ãŸã‚ã«ã€ãƒ™ã‚¯ãƒˆãƒ«ã‚’å¯è¦–åŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
    "å¤šæ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’å¯è¦–åŒ–ã™ã‚‹ãŸã‚ã«ã¯ã€æ¬¡å…ƒå‰Šæ¸›ã¨ã„ã†æ‰‹æ³•ã‚’ç”¨ã„ã¾ã™ã€‚PCA(ä¸»æˆåˆ†åˆ†æ)ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ãŒæŒã¤æƒ…å ±ã‚’ãªã‚‹ã¹ãæ¬ æã™ã‚‹ã“ã¨ãªãç¸®ç´„ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä»Šå›ã¯50æ¬¡å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’2æ¬¡å…ƒã«æ¸›ã‚‰ã—ã¾ã™ã€‚ã“ã‚Œã‚’å®Ÿè¡Œã™ã‚Œã°ã€matplotlibæ•£å¸ƒå›³ã‚’ä½¿ç”¨ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®æ¼”ç®—\n",
    "man = word_vectors[dictionary.index('man')]\n",
    "woman = word_vectors[dictionary.index('woman')]\n",
    "king = word_vectors[dictionary.index('king')]\n",
    "x = woman - man + king\n",
    "\n",
    "words = ['king', 'man', 'woman', 'queen']\n",
    "# GloVe è¾æ›¸ã‹ã‚‰æŒ‡å®šã—ãŸå˜èªã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–ã‚Šå‡ºã™\n",
    "vectors = [word_vectors[dictionary.index(word)] for word in words]\n",
    "\n",
    "# ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—çµæœã®ç¢ºèªã®ãŸã‚ã€é…åˆ—ã®æœ«å°¾ã«æ¼”ç®—çµæœxã‚’è¿½åŠ \n",
    "vectors.append(x)\n",
    "words.append('x')\n",
    "\n",
    "# 50æ¬¡å…ƒã‚’2æ¬¡å…ƒã«å‰Šæ¸›\n",
    "twodim = PCA(n_components=2).fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "# æ•£å¸ƒå›³ã‚’ä½œæˆ\n",
    "plt.scatter(twodim[:, 0], twodim[:, 1], edgecolors='k', c='b')\n",
    "\n",
    "for label, x, y in zip(words, twodim[:, 0], twodim[:, 1]):\n",
    "    labeltitle = label + \"(\" + str(np.round(x, decimals=1)) + \",\" + str(np.round(y, decimals=1)) + \")\"\n",
    "    plt.annotate(labeltitle, xy=(x, y), xytext=(5, 0), textcoords=\"offset points\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã€Œç‹ã€ã‹ã‚‰ã€Œç”·æ€§ã€ã‚’å¼•ã„ã¦ã€Œå¥³æ€§ã€ã‚’è¶³ã—ãŸã¨ãã€ãã‚Œã¯ã€Œå¥³ç‹ã€ã¨ãªã‚‹ã§ã‚ã‚ã†ã¨ã„ã†é¡æ¨ãŒã€GloVeãƒ™ã‚¯ãƒˆãƒ«ã®æ¼”ç®—ã§å¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚ã“ã‚Œã¯ä½•ã‚’æ„å‘³ã™ã‚‹ã‹ã¨ã„ã†ã¨ã€å˜èªé–“ã®æ„å‘³çš„ãªé–¢ä¿‚ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ç¾ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚ã“ã®æŠ€è¡“ã®ç™»å ´ã«ã‚ˆã£ã¦ã€å˜èªã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã«ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã€æ©Ÿæ¢°ç¿»è¨³ã‚„æ–‡ç« ã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºãªã©å¹…åºƒã„é ˜åŸŸã«ç”¨ã„ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n",
    "\n",
    "ãã‚Œã§ã¯ã‚‚ã£ã¨é¢ç™½ã„ç‰¹å¾´ã‚’ãŠè¦‹ã›ã—ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®æ¼”ç®—\n",
    "man = word_vectors[dictionary.index('man')]\n",
    "woman = word_vectors[dictionary.index('woman')]\n",
    "king = word_vectors[dictionary.index('king')]\n",
    "x = woman - man + king\n",
    "\n",
    "# GloVe è¾æ›¸å†…ã«å«ã¾ã‚Œã‚‹å˜èªã‚’ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«è¿½åŠ ï¼ˆè‡ªç”±ã«ç·¨é›†ã—ã¦ãã ã•ã„ï¼‰\n",
    "words = ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', \n",
    "         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
    "         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
    "         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
    "         'france', 'germany', 'hungary', 'luxembourg', 'australia', 'japan',\n",
    "         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
    "         'school', 'college', 'university', 'institute',\n",
    "         'king', 'man', 'woman', 'queen']\n",
    "\n",
    "# GloVe è¾æ›¸ã‹ã‚‰æŒ‡å®šã—ãŸå˜èªã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–ã‚Šå‡ºã™\n",
    "vectors = [word_vectors[dictionary.index(word)] for word in words]\n",
    "\n",
    "# ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—çµæœã®ç¢ºèªã®ãŸã‚ã€é…åˆ—ã®æœ«å°¾ã«æ¼”ç®—çµæœxã‚’è¿½åŠ \n",
    "vectors.append(x)\n",
    "words.append('x')\n",
    "\n",
    "# 50æ¬¡å…ƒã‚’2æ¬¡å…ƒã«å‰Šæ¸›\n",
    "twodim = PCA(n_components=2).fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "# æ•£å¸ƒå›³ã‚’ä½œæˆ\n",
    "plt.scatter(twodim[:, 0], twodim[:, 1], edgecolors='k', c='b')\n",
    "\n",
    "for label, x, y in zip(words, twodim[:, 0], twodim[:, 1]):\n",
    "    labeltitle = label + \"(\" + str(np.round(x, decimals=1)) + \",\" + str(np.round(y, decimals=1)) + \")\"\n",
    "    plt.annotate(labeltitle, xy=(x, y), xytext=(5, 0), textcoords=\"offset points\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¼¼ãŸæ„å‘³ã®å˜èªåŒå£«ãŒãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ä¸Šã§è¿‘ã„ä½ç½®ã«é…ç½®ã•ã‚Œã‚‹ã“ã¨ãŒã‚°ãƒ©ãƒ•ã‹ã‚‰åˆ†ã‹ã‚Šã¾ã—ãŸã‹ï¼Ÿã“ã‚Œã¯é­”æ³•ã®ã‚ˆã†ãªã“ã¨ã§ã™ï¼ğŸ§™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹\n",
    "\n",
    "Contoso Ltdã¯ã€ã‚¯ãƒ¬ãƒ¼ãƒ ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å—ã‘å–ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ä¾‹ã‚’å«ã‚€å°ã•ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ å½¼ã‚‰ã¯ã€ã“ã‚Œã‚’ã‚µãƒ³ãƒ—ãƒ«ã‚¯ãƒ¬ãƒ¼ãƒ ã”ã¨ã«1è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã§æä¾›ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ç¢ºèªã—ã¾ã™ã€‚ å°‘ã—æ™‚é–“ã‚’ã‹ã‘ã¦ã‚¯ãƒ¬ãƒ¼ãƒ ã‚’èª­ã‚“ã§ãã ã•ã„ï¼ˆã‚ãªãŸã¯ãã‚Œã‚‰ã®ã„ãã¤ã‹ã‚’ã‹ãªã‚Šã‚³ãƒŸã‚«ãƒ«ã«æ„Ÿã˜ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = './data'\n",
    "base_data_url = 'https://databricksdemostore.blob.core.windows.net/data/05.03/'\n",
    "filesToDownload = ['claims_text.txt', 'claims_labels.txt']\n",
    "\n",
    "os.makedirs(data_location, exist_ok=True)\n",
    "\n",
    "for file in filesToDownload:\n",
    "    data_url = os.path.join(base_data_url, file)\n",
    "    local_file_path = os.path.join(data_location, file)\n",
    "    urllib.request.urlretrieve(data_url, local_file_path)\n",
    "    print('Downloaded file: ', file)\n",
    "    \n",
    "claims_corpus = [claim for claim in open(os.path.join(data_location, 'claims_text.txt'))]\n",
    "claims_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚¯ãƒ¬ãƒ¼ãƒ ã‚µãƒ³ãƒ—ãƒ«ã«åŠ ãˆã¦ã€Contoso Ltdã¯ã€æä¾›ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã‚¯ãƒ¬ãƒ¼ãƒ ã®ãã‚Œãã‚Œã«0ï¼ˆã€Œä½å®…ä¿é™ºã‚¯ãƒ¬ãƒ¼ãƒ ã€ï¼‰ã¾ãŸã¯1ï¼ˆã€Œè‡ªå‹•è»Šä¿é™ºã‚¯ãƒ¬ãƒ¼ãƒ ã€ï¼‰ã®ã„ãšã‚Œã‹ã¨ã—ã¦ãƒ©ãƒ™ãƒ«ã‚’ä»˜ã‘ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚æä¾›ã—ã¦ã„ã¾ã™ã€‚ ã“ã‚Œã¯ã€ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«1è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦æç¤ºã•ã‚Œã€ã‚¯ãƒ¬ãƒ¼ãƒ ãƒ†ã‚­ã‚¹ãƒˆã¨åŒã˜é †åºã§æç¤ºã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€æä¾›ã•ã‚ŒãŸClaims_labels.txtãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª¿ã¹ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [int(re.sub(\"\\n\", \"\", label)) for label in open(os.path.join(data_location, 'claims_labels.txt'))]\n",
    "print(len(labels))\n",
    "print(labels[0:5]) # first 5 labels\n",
    "print(labels[-5:]) # last 5 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šè¨˜ã®å‡ºåŠ›ã‹ã‚‰ã‚ã‹ã‚‹ã‚ˆã†ã«ã€å€¤ã¯æ•´æ•°0ã¾ãŸã¯1ã§ã™ã€‚ã“ã‚Œã‚‰ã‚’ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹ãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ã“ã‚Œã‚‰ã®æ•´æ•°å€¤ã‚’ã‚«ãƒ†ã‚´ãƒªå€¤ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆä»–ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åˆ—æŒ™å‹ã®ã‚ˆã†ãªã‚‚ã®ã¨è€ƒãˆã¦ãã ã•ã„ï¼‰ è¨€èªï¼‰ã€‚\n",
    "\n",
    "`keras.utils` ã® to_categorical ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ã“ã‚Œã‚‰ã®å€¤ã‚’ãƒã‚¤ãƒŠãƒªã®ã‚«ãƒ†ã‚´ãƒªå€¤ã«å¤‰æ›ã§ãã¾ã™ã€‚ æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(labels, 2)\n",
    "print(labels.shape)\n",
    "print()\n",
    "print(labels[0:2]) # first 2 categorical labels\n",
    "print()\n",
    "print(labels[-2:]) # last 2 categorical labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚¯ãƒ¬ãƒ¼ãƒ ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ©ãƒ™ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚ŒãŸã®ã§ã€ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ—ãƒ­ã‚»ã‚¹ã®æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã€ã¤ã¾ã‚Šãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ã™ã‚‹æº–å‚™ãŒã§ãã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ã‚¯ãƒ¬ãƒ¼ãƒ ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’å‡¦ç†ã™ã‚‹\n",
    "\n",
    "- ã™ã¹ã¦ã®å˜èªã‚’å°æ–‡å­—ã«ã—ã¾ã™\n",
    "- åç¸®ã‚’æ‹¡å¼µã™ã‚‹ï¼ˆãŸã¨ãˆã°ã€ \"can't\" ãŒ \"cannot\" ã«ãªã‚‹ï¼‰\n",
    "- ç‰¹æ®Šæ–‡å­—ï¼ˆå¥èª­ç‚¹ãªã©ï¼‰ã‚’å‰Šé™¤ã™ã‚‹\n",
    "- ã‚¯ãƒ¬ãƒ¼ãƒ ãƒ†ã‚­ã‚¹ãƒˆå†…ã®å˜èªã®ãƒªã‚¹ãƒˆã‚’ã€è¾æ›¸å†…ã®ãã‚Œã‚‰ã®å˜èªã®å¯¾å¿œã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚ æ›¸é¢ã®ã‚¯ãƒ¬ãƒ¼ãƒ ã«è¡¨ç¤ºã•ã‚Œã‚‹å˜èªã®é †åºã¯ç¶­æŒã•ã‚Œã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€ã‚¯ãƒ¬ãƒ¼ãƒ ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’å‡¦ç†ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(token):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_token = pattern.sub('', token)\n",
    "    return filtered_token\n",
    "\n",
    "def convert_to_indices(corpus, dictionary, c_map, unk_word_index = 399999):\n",
    "    sequences = []\n",
    "    for i in range(len(corpus)):\n",
    "        tokens = corpus[i].split()\n",
    "        sequence = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word in c_map:\n",
    "                resolved_words = c_map[word].split()\n",
    "                for resolved_word in resolved_words:\n",
    "                    try:\n",
    "                        word_index = dictionary.index(resolved_word)\n",
    "                        sequence.append(word_index)\n",
    "                    except ValueError:\n",
    "                        sequence.append(unk_word_index) #Vector for unkown words\n",
    "            else:\n",
    "                try:\n",
    "                    clean_word = remove_special_characters(word)\n",
    "                    if len(clean_word) > 0:\n",
    "                        word_index = dictionary.index(clean_word)\n",
    "                        sequence.append(word_index)\n",
    "                except ValueError:\n",
    "                    sequence.append(unk_word_index) #Vector for unkown words\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "claims_corpus_indices = convert_to_indices(claims_corpus, dictionary, contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚¯ãƒ¬ãƒ¼ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç¢ºèªã™ã‚‹**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_special_characters(claims_corpus[5]).split())\n",
    "print()\n",
    "print('Ordered list of indices for the above claim')\n",
    "print(claims_corpus_indices[5])\n",
    "print('')\n",
    "print('For example, the index of second word in the claims text \\\"pedestrian\\\" is: ', dictionary.index('pedestrian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å›ºå®šé•·ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆã™ã‚‹**\n",
    "\n",
    "ã‚¯ãƒ¬ãƒ¼ãƒ ã§ä½¿ç”¨ã•ã‚Œã‚‹å˜èªã®æ•°ã¯ã€ã‚¯ãƒ¬ãƒ¼ãƒ ã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™ã€‚ å›ºå®šã‚µã‚¤ã‚ºã®å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ `keras.preprocessing.sequence` ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ` pad_sequences` ã‚’ä½¿ç”¨ã—ã¦ã€å˜èªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å›ºå®šã‚µã‚¤ã‚ºã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆã‚µã‚¤ã‚º= 125ï¼‰ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 125\n",
    "\n",
    "X = pad_sequences(claims_corpus_indices, maxlen=maxSeqLength, padding='pre', truncating='post')\n",
    "\n",
    "print('Review the new fixed size vector for a sample claim')\n",
    "print(remove_special_characters(claims_corpus[5]).split())\n",
    "print()\n",
    "print(X[5])\n",
    "print('')\n",
    "print('Lenght of the vector: ', len(X[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "\n",
    "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‹ã‚‰å…¥åŠ›ç‰¹å¾´ã‚’å‰å‡¦ç†ã—ãŸã®ã§ã€åˆ†é¡å™¨ã‚’ä½œæˆã™ã‚‹æº–å‚™ãŒã§ãã¾ã—ãŸã€‚ ã“ã®å ´åˆã€LSTMãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¯ã€å˜èªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’GloVeå˜èªãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹å˜èªåŸ‹ã‚è¾¼ã¿ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒã‚ã‚Šã¾ã™ã€‚ æ¬¡ã«ã€GloVeãƒ¯ãƒ¼ãƒ‰ãƒ™ã‚¯ãƒˆãƒ«ãŒLSTMå±¤ã«æ¸¡ã•ã‚Œã€ãã®å¾Œã«äºŒé …åˆ†é¡å‡ºåŠ›å±¤ãŒç¶šãã¾ã™ã€‚\n",
    "\n",
    "æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹é€ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(word_vectors.shape[0],\n",
    "                            word_vectors.shape[1],\n",
    "                            weights=[word_vectors],\n",
    "                            input_length=maxSeqLength,\n",
    "                            trainable=False)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åˆã«ã€ãƒ‡ãƒ¼ã‚¿ã‚’2ã¤ã®ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã™ï¼šï¼ˆ1ï¼‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã¨ï¼ˆ2ï¼‰æ¤œè¨¼ã¾ãŸã¯ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã€‚ æ¤œè¨¼ã‚»ãƒƒãƒˆã®ç²¾åº¦ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `Adam` æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚ ã¾ãŸã€å•é¡Œã®ã‚¿ã‚¤ãƒ—ãŒ `Binary Classification` ã§ã‚ã‚‹ãŸã‚ã€å‡ºåŠ›å±¤ã«ã¯ `Sigmoid` ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°ã‚’ä½¿ç”¨ã—ã€æå¤±é–¢æ•°ã«ã¯ `Binary Crossentropy` ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒ©ãƒ™ãƒ«ã«åˆã‚ã›ã¦DNNã«å­¦ç¿’ã•ã›ã‚‹æº–å‚™ãŒã§ãã¾ã—ãŸã€‚ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸã€‚\n",
    "\n",
    "æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 16\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å€¤ã€Œval_accuracyã€ã®æœ€çµ‚å‡ºåŠ›ã‚’è¦‹ã¦ãã ã•ã„ã€‚ ã“ã‚Œã¯ã€æ¤œè¨¼ã‚»ãƒƒãƒˆã®ç²¾åº¦ã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚ ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒ£ãƒ³ã‚¹ã‚’50ï¼…ã®ç²¾åº¦ã§ã‚ã‚‹ã¨è€ƒãˆã‚‹å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã¯ãƒ©ãƒ³ãƒ€ãƒ ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\n",
    "\n",
    "ã“ã®æ™‚ç‚¹ã§ãƒ©ãƒ³ãƒ€ãƒ ã‚ˆã‚Šã‚‚è‰¯ããªã„å ´åˆã§ã‚‚å•é¡Œã‚ã‚Šã¾ã›ã‚“-ã“ã‚Œã¯åˆã‚ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ï¼ å…¸å‹çš„ãªãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€æ¬¡ã®ã‚ˆã†ãªã•ã¾ã–ã¾ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç¹°ã‚Šè¿”ã—å®Ÿè¡Œã—ã¾ã™ã€‚\n",
    "- ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«ã•ã‚‰ã«ãƒ©ãƒ™ãƒ«ãŒä»˜ã‘ã‚‰ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã™ã‚‹\n",
    "- éé©åˆã‚’é˜²ããŸã‚ã®æ­£å‰‡åŒ–\n",
    "- ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚ãŸã‚Šã®ãƒãƒ¼ãƒ‰æ•°ã€å­¦ç¿’ç‡ãªã©ã®ãƒ¢ãƒ‡ãƒ«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®èª¿æ•´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åˆ†é¡ã‚¯ãƒ¬ãƒ¼ãƒ ã®ãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ãŸã®ã§ã€ä¸€é€£ã®ã‚¯ãƒ¬ãƒ¼ãƒ ã«å¯¾ã—ã¦ãã‚Œã‚’è©¦ã—ã¦ãã ã•ã„ã€‚ æœ€åˆã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã‚’æ€ã„å‡ºã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_claim = ['I crashed my car into a pole.', \n",
    "              'The flood ruined my house.', \n",
    "              'I lost control of my car and fell in the river.']\n",
    "\n",
    "test_claim_indices = convert_to_indices(test_claim, dictionary, contractions)\n",
    "test_data = pad_sequences(test_claim_indices, maxlen=maxSeqLength, padding='pre', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦åˆ†é¡ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)\n",
    "pred_label = pred.argmax(axis=1)\n",
    "pred_df = pd.DataFrame(np.column_stack((pred,pred_label)), columns=['class_0', 'class_1', 'label'])\n",
    "pred_df.label = pred_df.label.astype(int)\n",
    "print('Predictions')\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "\n",
    "ã“ã‚Œã§ä½œæ¥­ãƒ¢ãƒ‡ãƒ«ãŒã§ããŸã®ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¦ã€ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚ŒãŸWebã‚µãƒ¼ãƒ“ã‚¹ãŒãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "*æ¬¡ã®2ã¤ã®ã‚»ãƒ«ã®å®Ÿè¡Œã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™*\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã«ã¯ã€æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "output_folder = './output'\n",
    "model_filename = 'final_model.hdf5'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "model.save(os.path.join(output_folder, model_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŒã˜Notebookã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¸ã®ãƒ¢ãƒ‡ãƒ«ã®å†ãƒ­ãƒ¼ãƒ‰ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã«ã¯ã€æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = load_model(os.path.join(output_folder, model_filename))\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‰ã¨åŒã˜ã‚ˆã†ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚\n",
    "\n",
    "æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€å†ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã‚’è©¦ã¿ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = loaded_model.predict(test_data)\n",
    "pred_label = pred.argmax(axis=1)\n",
    "pred_df = pd.DataFrame(np.column_stack((pred,pred_label)), columns=['class_0', 'class_1', 'label'])\n",
    "pred_df.label = pred_df.label.astype(int)\n",
    "print('Predictions')\n",
    "pred_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python36",
   "display_name": "Python 3.6",
   "language": "python"
  },
  "language_info": {
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6",
   "file_extension": ".py",
   "codemirror_mode": {
    "version": 3,
    "name": "ipython"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}