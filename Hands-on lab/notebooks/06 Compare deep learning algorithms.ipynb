{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kerasによるクレーム分類：Pythonディープラーニングライブラリ補講\n",
    "## SimpleRNN/GRU/LSTMの比較\n",
    "\n",
    "このノートブックでは、複数のテキスト分類アルゴリズムを使ってモデルの精度や学習時間、推論速度の比較をすることができます。Kerasで簡単に試すことができるアルゴリズムとして、LSTM のほかに SimpleRNN と GRU があります。\n",
    "\n",
    "このノートブックでは、以下で構成されるテキスト分析プロセスについて説明します。\n",
    "\n",
    "- グローブ語の埋め込みを使用した例の類推\n",
    "- GloVe単語の埋め込みを使用したトレーニングデータのベクトル化\n",
    "- SimpleRNN/GRU/LSTMベースの分類モデルの作成とトレーニング\n",
    "- モデルを使用して分類を予測する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モジュールを準備する\n",
    "\n",
    "このノートブックは、Kerasライブラリを使用して分類器を作成およびトレーニングします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, LSTM, GRU, SimpleRNN\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "print('Keras version: ', keras.__version__)\n",
    "print('Tensorflow version: ', tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**事前トレーニング済みのGloVe単語の埋め込みをダウンロードして、このノートブックにロードしてみましょう。**\n",
    "\n",
    "これで、サイズが **400,000** ワードの `dictionary`と、辞書内の単語に対応する ` GloVe wordベクトル ` が作成されます。 各単語ベクトルのサイズは50であるため、ここで使用される単語埋め込みの次元は **50** です。\n",
    "\n",
    "*次のセルの実行には数分かかる場合があります*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                  'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordsList.npy')\n",
    "\n",
    "word_vectors_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordVectors.npy')\n",
    "\n",
    "word_vectors_dir = './word_vectors'\n",
    "\n",
    "if not (os.path.exists(os.path.join(word_vectors_dir, 'wordsList.npy'))):\n",
    "    os.makedirs(word_vectors_dir, exist_ok=True)\n",
    "    urllib.request.urlretrieve(words_list_url, os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "else:\n",
    "    dictionary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "    dictionary = dictionary.tolist()\n",
    "    dictionary = [word.decode('UTF-8') for word in dictionary]\n",
    "    print('Loaded the dictionary! Dictionary size: ', len(dictionary))\n",
    "\n",
    "if not (os.path.exists(os.path.join(word_vectors_dir, 'wordVectors.npy'))):\n",
    "    urllib.request.urlretrieve(word_vectors_url, os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "else:\n",
    "    word_vectors = np.load(os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "    print ('Loaded the word vectors! Shape of the word vectors: ', word_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**収縮マップという単語を作成します。 マップはコーパスの縮約を拡張するために使用されます（たとえば、 \"can't\" は\"cannot\"になります）。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/contractions.xlsx')\n",
    "\n",
    "if not (os.path.exists('./contractions/contractions.xlsx')):\n",
    "    contractions_df = pd.read_excel(contractions_url)\n",
    "else:\n",
    "    contractions_df = pd.read_excel('./contractions/contractions.xlsx')\n",
    "    \n",
    "contractions = dict(zip(contractions_df.original, contractions_df.expanded))\n",
    "print('Review first 10 entries from the contractions map')\n",
    "print(contractions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレーニングデータを準備する\n",
    "\n",
    "Contoso Ltdは、クレームテキストとして受け取るテキストの例を含む小さなドキュメントを提供しています。 彼らは、これをサンプルクレームごとに1行のテキストファイルで提供しています。\n",
    "\n",
    "次のセルを実行して、ファイルの内容をダウンロードして確認します。 少し時間をかけてクレームを読んでください（あなたはそれらのいくつかをかなりコミカルに感じるかもしれません！）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = './data'\n",
    "base_data_url = 'https://databricksdemostore.blob.core.windows.net/data/05.03/'\n",
    "filesToDownload = ['claims_text.txt', 'claims_labels.txt']\n",
    "\n",
    "claims_corpus = [claim for claim in open(os.path.join(data_location, 'claims_text.txt'))]\n",
    "claims_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クレームサンプルに加えて、Contoso Ltdは、提供されたサンプルクレームのそれぞれに0（「住宅保険クレーム」）または1（「自動車保険クレーム」）のいずれかとしてラベルを付けるドキュメントも提供しています。 これは、サンプルごとに1行のテキストファイルとして提示され、クレームテキストと同じ順序で提示されます。\n",
    "\n",
    "次のセルを実行して、提供されたClaims_labels.txtファイルの内容を調べます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [int(re.sub(\"\\n\", \"\", label)) for label in open(os.path.join(data_location, 'claims_labels.txt'))]\n",
    "print(len(labels))\n",
    "print(labels[0:5]) # first 5 labels\n",
    "print(labels[-5:]) # last 5 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の出力からわかるように、値は整数0または1です。これらをモデルのトレーニングに使用するラベルとして使用するには、これらの整数値をカテゴリ値に変換する必要があります（他のプログラミングの列挙型のようなものと考えてください） 言語）。\n",
    "\n",
    "`keras.utils` の to_categorical メソッドを使用して、これらの値をバイナリのカテゴリ値に変換できます。 次のセルを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(labels, 2)\n",
    "print(labels.shape)\n",
    "print()\n",
    "print(labels[0:2]) # first 2 categorical labels\n",
    "print()\n",
    "print(labels[-2:]) # last 2 categorical labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クレームテキストとラベルが読み込まれたので、テキスト分析プロセスの最初のステップ、つまりテキストを正規化する準備ができました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クレームコーパスを処理する\n",
    "\n",
    "- すべての単語を小文字にします\n",
    "- 収縮を拡張する（たとえば、 \"can't\" が \"cannot\" になる）\n",
    "- 特殊文字（句読点など）を削除する\n",
    "- クレームテキスト内の単語のリストを、辞書内のそれらの単語の対応するインデックスのリストに変換します。 書面のクレームに表示される単語の順序は維持されることに注意してください。\n",
    "\n",
    "次のセルを実行して、クレームコーパスを処理します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(token):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_token = pattern.sub('', token)\n",
    "    return filtered_token\n",
    "\n",
    "def convert_to_indices(corpus, dictionary, c_map, unk_word_index = 399999):\n",
    "    sequences = []\n",
    "    for i in range(len(corpus)):\n",
    "        tokens = corpus[i].split()\n",
    "        sequence = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word in c_map:\n",
    "                resolved_words = c_map[word].split()\n",
    "                for resolved_word in resolved_words:\n",
    "                    try:\n",
    "                        word_index = dictionary.index(resolved_word)\n",
    "                        sequence.append(word_index)\n",
    "                    except ValueError:\n",
    "                        sequence.append(unk_word_index) #Vector for unkown words\n",
    "            else:\n",
    "                try:\n",
    "                    clean_word = remove_special_characters(word)\n",
    "                    if len(clean_word) > 0:\n",
    "                        word_index = dictionary.index(clean_word)\n",
    "                        sequence.append(word_index)\n",
    "                except ValueError:\n",
    "                    sequence.append(unk_word_index) #Vector for unkown words\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "claims_corpus_indices = convert_to_indices(claims_corpus, dictionary, contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**固定長ベクトルを作成する**\n",
    "\n",
    "クレームで使用される単語の数は、クレームによって異なります。 固定サイズの入力ベクトルを作成する必要があります。 `keras.preprocessing.sequence` のユーティリティ関数 ` pad_sequences` を使用して、単語インデックスの固定サイズのベクトル（サイズ= 125）を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 125\n",
    "\n",
    "X = pad_sequences(claims_corpus_indices, maxlen=maxSeqLength, padding='pre', truncating='post')\n",
    "\n",
    "print('Review the new fixed size vector for a sample claim')\n",
    "print(remove_special_characters(claims_corpus[5]).split())\n",
    "print()\n",
    "print(X[5])\n",
    "print('')\n",
    "print('Lenght of the vector: ', len(X[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SimpleRNNを構築する\n",
    "\n",
    "テキストデータのトレーニングから入力特徴を前処理したので、分類器を作成する準備ができました。 この場合、SimpleRNNを構築します。 ネットワークには、単語インデックスをGloVe単語ベクトルに変換する単語埋め込みレイヤーがあります。 次に、GloVeワードベクトルがSimpleRNN層に渡され、その後にバイナリ分類出力層が続きます。\n",
    "\n",
    "<img src=\"../media/simplernn.png\" >\n",
    "\n",
    "SimpleRNNはLSTMと比べて回路がかなり単純です。処理中に前の隠れ状態を次の単語ステップに渡します。隠れ状態は、ニューラルネットワークの記憶として機能します。ネットワークが以前に見た以前のデータに関する情報を保持します。回路内部での計算は**tanh**で値の調整くらいしかしていませんが、短い文章ではかなりうまく動作します。\n",
    "\n",
    "次のセルを実行して、ニューラルネットワークの構造を構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(word_vectors.shape[0],\n",
    "                            word_vectors.shape[1],\n",
    "                            weights=[word_vectors],\n",
    "                            input_length=maxSeqLength,\n",
    "                            trainable=False)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(SimpleRNN(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークをトレーニングする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初に、データを2つのセットに分割します：（1）トレーニングセットと（2）検証またはテストセット。 検証セットの精度は、モデルのパフォーマンスを測定するために使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `Adam` 最適化アルゴリズムを使用してモデルをトレーニングします。 また、問題のタイプが `Binary Classification` であるため、出力層には `Sigmoid` アクティベーション関数を使用し、損失関数には `Binary Crossentropy` を使用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、トレーニングデータとラベルに合わせてSimpleRNNに学習させる準備ができました。 トレーニングのバッチサイズとエポック数を定義しました。\n",
    "\n",
    "次のセルを実行して、モデルをデータに合わせます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# EaelyStoppingの設定\n",
    "early_stopping =  EarlyStopping(\n",
    "                            monitor='val_loss',\n",
    "                            min_delta=0.0,\n",
    "                            patience=2\n",
    "    )\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test),callbacks=[early_stopping])\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleRNNモデルの精度を出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('loss=', score[0])\n",
    "print('accuracy=', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleRNNで分類クレームのテスト\n",
    "\n",
    "モデルを構築したので、一連のクレームに対してそれを試してください。 最初にテキストを前処理する必要があることを思い出してください。\n",
    "\n",
    "次のセルを実行して、テストデータを準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_claim = ['I crashed my car into a pole.', \n",
    "              'The flood ruined my house.', \n",
    "              'I lost control of my car and fell in the river.']\n",
    "\n",
    "test_claim_indices = convert_to_indices(test_claim, dictionary, contractions)\n",
    "test_data = pad_sequences(test_claim_indices, maxlen=maxSeqLength, padding='pre', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、モデルを使用して分類を予測します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)\n",
    "pred_label = pred.argmax(axis=1)\n",
    "pred_df = pd.DataFrame(np.column_stack((pred,pred_label)), columns=['class_0', 'class_1', 'label'])\n",
    "pred_df.label = pred_df.label.astype(int)\n",
    "print('Predictions')\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまでのセッションとmodelをクリアします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GRU\n",
    "\n",
    "GRU(Gated recurrent unit)は新世代の再帰型ニューラルネットワークであり、LSTMによく似ています。GRUはセル状態を取り除き、隠れ状態を使用して情報を転送します。また、リセットゲートと更新ゲートの2つのゲートしかありません。GRUはLSTMに比べ、行列演算が少なくLSTMを学習するよりもいくらか高速です。ただし、精度についてはLSTMとGRUでは明確な差はありませんので、シーンによってどちらも検討するようにしてください。\n",
    "\n",
    "<img src=\"../media/gru.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(word_vectors.shape[0],\n",
    "                            word_vectors.shape[1],\n",
    "                            weights=[word_vectors],\n",
    "                            input_length=maxSeqLength,\n",
    "                            trainable=False)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# EaelyStoppingの設定\n",
    "early_stopping =  EarlyStopping(\n",
    "                            monitor='val_loss',\n",
    "                            min_delta=0.0,\n",
    "                            patience=2\n",
    "    )\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test),callbacks=[early_stopping])\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRUモデルの精度を出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('loss=', score[0])\n",
    "print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRUで分類クレームのテスト\n",
    "\n",
    "モデルを構築したので、一連のクレームに対してそれを試してください。 最初にテキストを前処理する必要があることを思い出してください。\n",
    "\n",
    "次のセルを実行して、テストデータを準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_claim = ['I crashed my car into a pole.', \n",
    "              'The flood ruined my house.', \n",
    "              'I lost control of my car and fell in the river.']\n",
    "\n",
    "test_claim_indices = convert_to_indices(test_claim, dictionary, contractions)\n",
    "test_data = pad_sequences(test_claim_indices, maxlen=maxSeqLength, padding='pre', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、モデルを使用して分類を予測します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)\n",
    "pred_label = pred.argmax(axis=1)\n",
    "pred_df = pd.DataFrame(np.column_stack((pred,pred_label)), columns=['class_0', 'class_1', 'label'])\n",
    "pred_df.label = pred_df.label.astype(int)\n",
    "print('Predictions')\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまでのセッションとmodelをクリアします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LSTM\n",
    "\n",
    "LSTM(Long short-term memory)は再帰型ニューラルネットワークの中で、最も回路が複雑です。LSTMは、セルの状態と、さまざまなゲートから成り立っています。これらは、LSTMが情報を保持または忘れることを可能にするために使用されます。最も大きな特長は、従来のRNNでは学習できなかった長期依存(long-term dependencies)を学習可能であるという点です。SimpleRNNのような構造だと、長い文章を学習することができませんでした（勾配消失問題）。LSTMはこの問題を解決しています。\n",
    "\n",
    "<img src=\"../media/lstm.png\" />\n",
    "\n",
    "LSTMは入力として通常の入力と、一つ前の隠れ層からの入力の2つがあります。この2つの入力によって、セルの状態と3つのゲートの値が更新されます。処理のプロセスは以下の通りです。\n",
    "1. ①忘却ゲート(forget gate)でどの情報を破棄または保持するかを決定します。値は0から1の間で出てきます。0に近いほど忘れることを意味し、1に近いほど保持することを意味します。\n",
    "2. 過去の状態を保持する機能を持つ、セル状態(cell state)が更新されます。\n",
    "3. ②入力ゲート(input gate)は以前の隠れ状態と現在の入力をシグモイド関数に渡します。これにより、値を0と1の間で変換することによって更新される値が決まります。0は重要ではないことを意味し、1は重要であることを意味します。\n",
    "4. ③出力ゲート(output gate)は、以前の隠れ状態と現在の入力をシグモイド関数に渡します。次に、新しく変更されたセル状態をtanh関数に渡します。tanhの出力とシグモイドの出力を乗算して、隠れ状態がどの情報を伝達するかを決定します。シグモイド関数によって、重要な情報であると判断すれば、更新されたセル状態の値を次に引き継ぐということです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(word_vectors.shape[0],\n",
    "                            word_vectors.shape[1],\n",
    "                            weights=[word_vectors],\n",
    "                            input_length=maxSeqLength,\n",
    "                            trainable=False)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# EaelyStoppingの設定\n",
    "early_stopping =  EarlyStopping(\n",
    "                            monitor='val_loss',\n",
    "                            min_delta=0.0,\n",
    "                            patience=2\n",
    "    )\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test),callbacks=[early_stopping])\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMモデルの精度を出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('loss=', score[0])\n",
    "print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMで分類クレームのテスト\n",
    "\n",
    "モデルを構築したので、一連のクレームに対してそれを試してください。 最初にテキストを前処理する必要があることを思い出してください。\n",
    "\n",
    "次のセルを実行して、テストデータを準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_claim = ['I crashed my car into a pole.', \n",
    "              'The flood ruined my house.', \n",
    "              'I lost control of my car and fell in the river.']\n",
    "\n",
    "test_claim_indices = convert_to_indices(test_claim, dictionary, contractions)\n",
    "test_data = pad_sequences(test_claim_indices, maxlen=maxSeqLength, padding='pre', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、モデルを使用して分類を予測します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)\n",
    "pred_label = pred.argmax(axis=1)\n",
    "pred_df = pd.DataFrame(np.column_stack((pred,pred_label)), columns=['class_0', 'class_1', 'label'])\n",
    "pred_df.label = pred_df.label.astype(int)\n",
    "print('Predictions')\n",
    "pred_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python36",
   "display_name": "Python 3.6",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}