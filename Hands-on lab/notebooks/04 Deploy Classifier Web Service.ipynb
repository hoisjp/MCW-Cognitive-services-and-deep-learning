{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KerasモデルをONNXに変換する\n",
    "\n",
    "次の手順では、トレーニングしたばかりのKerasモデルをONNX形式に変換します。 これにより、次のようなAzure Databricks以外の非常に幅広い環境でこのモデルを分類に使用できるようになります。\n",
    "\n",
    "- ウェブサービス\n",
    "- iOSおよびAndroidモバイルアプリ\n",
    "- Windowsアプリ\n",
    "- IoTデバイス\n",
    "\n",
    "さらに、ONNXランタイムとライブラリは、業界最高のハードウェアの一部でパフォーマンスを最大化するように設計されています。 このラボでは、ONNXモデルとKerasモデルの推論パフォーマンスを比較します。\n",
    "\n",
    "まず、トレーニング済みのKerasモデルをファイルからロードしてから、モデルをONNXに変換します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kerasモデルを読み込む\n",
    "\n",
    "保存したKerasモデルを読み込みます。 KerasモデルをONNX形式に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(125)\n",
    "\n",
    "from keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "output_folder = './output'\n",
    "model_filename = 'final_model.hdf5'\n",
    "\n",
    "keras_model = load_model(os.path.join(output_folder, model_filename))\n",
    "print(keras_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNXに変換\n",
    "\n",
    "ロードされたKerasモデルをONNX形式に変換し、ONNXモデルをデプロイメントフォルダーに保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxmltools\n",
    "\n",
    "deployment_folder = 'deploy'\n",
    "onnx_export_folder = 'onnx'\n",
    "\n",
    "# Convert the Keras model to ONNX\n",
    "onnx_model_name = 'claim_classifier.onnx'\n",
    "converted_model = onnxmltools.convert_keras(keras_model, onnx_model_name, target_opset=7)\n",
    "\n",
    "# Save the model locally...\n",
    "onnx_model_path = os.path.join(deployment_folder, onnx_export_folder)\n",
    "os.makedirs(onnx_model_path, exist_ok=True)\n",
    "onnxmltools.utils.save_model(converted_model, os.path.join(onnx_model_path,onnx_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNXモデルを使用した推論\n",
    "\n",
    "- ONNXランタイム InferenceSession を作成する\n",
    "- 予想される入力形状を確認して推論を行う\n",
    "- テストデータを準備する\n",
    "- テストデータでONNXとKerasモデルの両方を使用して推論する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Runtime InferenceSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "# Load the ONNX model and observe the expected input shape\n",
    "onnx_session = onnxruntime.InferenceSession(\n",
    "    os.path.join(os.path.join(deployment_folder, onnx_export_folder), onnx_model_name))\n",
    "input_name = onnx_session.get_inputs()[0].name\n",
    "output_name = onnx_session.get_outputs()[0].name\n",
    "print('Expected input shape: ', onnx_session.get_inputs()[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストデータを準備する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GloVe単語ベクトルを読み込む**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_dir = './word_vectors'\n",
    "\n",
    "dictonary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "dictonary = dictonary.tolist()\n",
    "dictonary = [word.decode('UTF-8') for word in dictonary]\n",
    "print('Loaded the dictonary! Dictonary size: ', len(dictonary))\n",
    "\n",
    "word_vectors = np.load(os.path.join(word_vectors_dir, 'wordVectors.npy'))\n",
    "print ('Loaded the word vectors! Shape of the word vectors: ', word_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**単語収縮マップを作成する**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/contractions.xlsx')\n",
    "contractions_df = pd.read_excel(contractions_url)\n",
    "contractions = dict(zip(contractions_df.original, contractions_df.expanded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**テストデータを処理するヘルパー関数を設定する**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def remove_special_characters(token):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_token = pattern.sub('', token)\n",
    "    return filtered_token\n",
    "\n",
    "def convert_to_indices(corpus, dictonary, c_map, unk_word_index = 399999):\n",
    "    sequences = []\n",
    "    for i in range(len(corpus)):\n",
    "        tokens = corpus[i].split()\n",
    "        sequence = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word in c_map:\n",
    "                resolved_words = c_map[word].split()\n",
    "                for resolved_word in resolved_words:\n",
    "                    try:\n",
    "                        word_index = dictonary.index(resolved_word)\n",
    "                        sequence.append(word_index)\n",
    "                    except ValueError:\n",
    "                        sequence.append(unk_word_index) #Vector for unkown words\n",
    "            else:\n",
    "                try:\n",
    "                    clean_word = remove_special_characters(word)\n",
    "                    if len(clean_word) > 0:\n",
    "                        word_index = dictonary.index(clean_word)\n",
    "                        sequence.append(word_index)\n",
    "                except ValueError:\n",
    "                    sequence.append(unk_word_index) #Vector for unkown words\n",
    "        sequences.append(sequence)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**テストデータを前処理する**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxSeqLength = 125\n",
    "\n",
    "test_claim = ['I crashed my car into a pole.']\n",
    "\n",
    "test_claim_indices = convert_to_indices(test_claim, dictonary, contractions)\n",
    "test_data = pad_sequences(test_claim_indices, maxlen=maxSeqLength, padding='pre', truncating='post')\n",
    "\n",
    "# convert the data type to float\n",
    "test_data_float = np.reshape(test_data.astype(np.float32), (1,maxSeqLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論する\n",
    "\n",
    "テストデータでONNXとKerasモデルの両方を使用して推論を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an ONNX session to classify the sample.\n",
    "print('ONNX prediction: ', onnx_session.run([output_name], {input_name : test_data_float}))\n",
    "\n",
    "# Use Keras to make predictions on the same sample\n",
    "print('Keras prediction: ', keras_model.predict(test_data_float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論パフォーマンスの比較：ONNXとKeras\n",
    "\n",
    "同じサンプルを1,000回実行して、ONNXとKerasのパフォーマンスを評価します。 次の3つのセルを実行し、環境でのパフォーマンスを比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will compare the performance of ONNX vs Keras\n",
    "import timeit\n",
    "n = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "for i in range(n):\n",
    "    keras_model.predict(test_data_float)\n",
    "keras_elapsed = timeit.default_timer() - start_time\n",
    "print('Keras performance: ', keras_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "for i in range(n):\n",
    "    onnx_session.run([output_name], {input_name : test_data_float})\n",
    "onnx_elapsed = timeit.default_timer() - start_time\n",
    "print('ONNX performance: ', onnx_elapsed)\n",
    "print('ONNX is about {} times faster than Keras'.format(round(keras_elapsed/onnx_elapsed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNXモデルをAzure Container Instance（ACI）にデプロイする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learningワークスペースを作成して接続する\n",
    "\n",
    "前のノートブックに保存されたワークスペース構成ファイルを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat .azureml/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**保存された設定ファイルから `Workspace` を作成します**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(azureml.core.VERSION)\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learningにモデルを登録する\n",
    "\n",
    "以下では、モデルをAzure Machine Learningに登録します（これにより、コピーがクラウドに保存されます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register the model and vectorizer\n",
    "from azureml.core.model import Model\n",
    "\n",
    "registered_model_name = 'claim_classifier_onnx'\n",
    "onnx_model_path = os.path.join(os.path.join(deployment_folder, onnx_export_folder), onnx_model_name)\n",
    "\n",
    "registered_model = Model.register(model_path = onnx_model_path, # this points to a local file\n",
    "                       model_name = registered_model_name, # this is the name the model is registered with         \n",
    "                       description = \"Claims classification model.\",\n",
    "                       workspace = ws)\n",
    "\n",
    "print(registered_model.name, registered_model.description, registered_model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スコアリングWebサービスを作成する\n",
    "\n",
    "Azure Machine Learningサービスでスコアリング用のモデルをデプロイする場合、モデルをロードしてスコアリングに使用する単純なWebサービスのコードを定義する必要があります。 慣例により、このサービスには、モデルをロードする2つのメソッドinitと、ロードされたモデルを使用してデータをスコアリングするrunがあります。\n",
    "\n",
    "このスコアリングサービスコードは、特別に準備されたDockerコンテナー内に後でデプロイされます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**スコアリングWebサービスのPythonファイルを保存する**\n",
    "\n",
    "スコアリングWebサービスには、登録されたモデル、つまり推論を行うためのONNXモデルが必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scoring_service.py\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import json\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from azureml.core.model import Model\n",
    "import onnxruntime\n",
    "\n",
    "def init():\n",
    "\n",
    "    global onnx_session\n",
    "    global dictonary\n",
    "    global contractions\n",
    "    \n",
    "    try:\n",
    "        words_list_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                          'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/wordsList.npy')\n",
    "        word_vectors_dir = './word_vectors'\n",
    "        os.makedirs(word_vectors_dir, exist_ok=True)\n",
    "        urllib.request.urlretrieve(words_list_url, os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "        dictonary = np.load(os.path.join(word_vectors_dir, 'wordsList.npy'))\n",
    "        dictonary = dictonary.tolist()\n",
    "        dictonary = [word.decode('UTF-8') for word in dictonary]\n",
    "        print('Loaded the dictonary! Dictonary size: ', len(dictonary))\n",
    "        \n",
    "        contractions_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                            'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/glove50d/contractions.xlsx')\n",
    "        contractions_df = pd.read_excel(contractions_url)\n",
    "        contractions = dict(zip(contractions_df.original, contractions_df.expanded))\n",
    "        print('Loaded contractions!')\n",
    "        \n",
    "        # Retrieve the path to the model file using the model name\n",
    "        onnx_model_name = 'claim_classifier_onnx'\n",
    "        onnx_model_path = Model.get_model_path(onnx_model_name)\n",
    "        print('onnx_model_path: ', onnx_model_path)\n",
    "        \n",
    "        onnx_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "        print('Onnx Inference Session Created!')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def remove_special_characters(token):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_token = pattern.sub('', token)\n",
    "    return filtered_token\n",
    "\n",
    "def convert_to_indices(corpus, dictonary, c_map, unk_word_index = 399999):\n",
    "    sequences = []\n",
    "    for i in range(len(corpus)):\n",
    "        tokens = corpus[i].split()\n",
    "        sequence = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word in c_map:\n",
    "                resolved_words = c_map[word].split()\n",
    "                for resolved_word in resolved_words:\n",
    "                    try:\n",
    "                        word_index = dictonary.index(resolved_word)\n",
    "                        sequence.append(word_index)\n",
    "                    except ValueError:\n",
    "                        sequence.append(unk_word_index) #Vector for unkown words\n",
    "            else:\n",
    "                try:\n",
    "                    clean_word = remove_special_characters(word)\n",
    "                    if len(clean_word) > 0:\n",
    "                        word_index = dictonary.index(clean_word)\n",
    "                        sequence.append(word_index)\n",
    "                except ValueError:\n",
    "                    sequence.append(unk_word_index) #Vector for unkown words\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        print(\"Received input: \", raw_data)\n",
    "        \n",
    "        maxSeqLength = 125\n",
    "        \n",
    "        print('Processing input...')\n",
    "        input_data_raw = np.array(json.loads(raw_data))\n",
    "        input_data_indices = convert_to_indices(input_data_raw, dictonary, contractions)\n",
    "        input_data_padded = pad_sequences(input_data_indices, maxlen=maxSeqLength, padding='pre', truncating='post')\n",
    "        # convert the data type to float\n",
    "        input_data = np.reshape(input_data_padded.astype(np.float32), (1,maxSeqLength))\n",
    "        print('Done processing input.')\n",
    "        \n",
    "        # Run an ONNX session to classify the input.\n",
    "        result = onnx_session.run(None, {onnx_session.get_inputs()[0].name: input_data})[0].argmax(axis=1).item()\n",
    "        # return just the classification index (0 or 1)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        error = str(e)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルのパッケージ化とACIへのデプロイ\n",
    "\n",
    "スコアリングサービスは、Conda環境ファイルを使用して依存関係をインストールできます。 このファイルにリストされている項目は、作成されたDockerコンテナー内にインストールされているcondaまたはpipであり、スコアリングWebサービスロジックで使用できます。\n",
    "\n",
    "推奨されるデプロイパターンは、 `deploy_configuration` メソッドでデプロイ設定オブジェクトを作成してから、以下のように[Model](https://docs.microsoft.com/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py) クラスのdeployメソッドで使用することです。 この場合、 `AciWebservice` の `deploy_configuration` を使用して、CPUコアとメモリサイズを指定します。\n",
    "\n",
    "Webサービスの準備ができると、次のような出力が表示されます。 `Succeeded - ACI service creation operation finished, operation \"Succeeded\"`\n",
    "\n",
    "次のセルを実行します。 完了するまでに5〜10分かかる場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Conda dependencies environment file\n",
    "print(\"Creating conda dependencies file locally...\")\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "conda_packages = ['numpy==1.16.4', 'xlrd==1.2.0', 'pandas==0.25.1', 'scikit-learn==0.21.3']\n",
    "pip_packages = ['azureml-defaults', 'azureml-sdk', 'tensorflow==1.13.1', 'keras==2.3.1', 'onnxruntime==1.0.0']\n",
    "\n",
    "environment = Environment('my-environment')\n",
    "environment.python.conda_dependencies = CondaDependencies.create(conda_packages=conda_packages, pip_packages=pip_packages)\n",
    "\n",
    "execution_script = 'scoring_service.py'\n",
    "service_name = \"claimclassservice\"\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=execution_script, environment=environment)\n",
    "aci_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=1,\n",
    "    memory_gb=1, \n",
    "    tags = {'name': 'Claim Classification'}, \n",
    "    description = \"Classifies a claim as home or auto.\")\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                      name=service_name,\n",
    "                      models=[registered_model],\n",
    "                      inference_config=inference_config,\n",
    "                      deployment_config=aci_config)\n",
    "\n",
    "# wait for the deployment to finish\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデプロイ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サービスオブジェクトを直接呼び出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_claims = ['I crashed my car into a pole.', \n",
    "               'The flood ruined my house.', \n",
    "               'I lost control of my car and fell in the river.']\n",
    "\n",
    "for i in range(len(test_claims)):\n",
    "    result = service.run(json.dumps([test_claims[i]]))\n",
    "    print('Predicted label for test claim #{} is {}'.format(i+1, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### デプロイされたWebサービスをテストするためのHTTP呼び出しを行う\n",
    "\n",
    "RESTクライアントからサービスを呼び出すには、スコアリングURIを取得する必要があります。 出力されたスコアリングURIをメモしてください。最後のノートブックで必要になります。\n",
    "\n",
    "このサービスのデプロイに使用されるデフォルト設定では、認証を必要としないサービスが作成されるため、このサービスを呼び出すために必要な値はスコアリングURIのみです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = service.scoring_uri\n",
    "print('ACI Service: Claim Classification scoring URI is: {}'.format(url))\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "for i in range(len(test_claims)):\n",
    "    response = requests.post(url, json.dumps([test_claims[i]]), headers=headers)\n",
    "    print('Predicted label for test claim #{} is {}'.format(i+1, response.text))"
   ]
  }
 ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
 "nbformat": 4,
 "nbformat_minor": 2
}